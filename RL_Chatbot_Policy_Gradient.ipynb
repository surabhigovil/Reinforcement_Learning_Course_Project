{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Chatbot_Policy_Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8HkXkCKJXhB"
      },
      "source": [
        "# Sequence to Sequence Chatbot using Policy Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE7AcHkwcqII",
        "outputId": "33cc5d02-9fb4-442f-ff6e-e45b6096d868"
      },
      "source": [
        "!pip install torch==1.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torch-1.6.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (552.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 552.8 MB 4.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.6.0+cu92 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0+cu92 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0+cu92 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0+cu92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_fNbxymdGRG"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "#device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvPfEZYFdGRG",
        "outputId": "498de22c-2bbe-4901-c282-cb5e2153e24d"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6.0+cu92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2IG9hteJjMj"
      },
      "source": [
        "## Data Downloading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdVGT8sazTya",
        "outputId": "315d79e4-bfea-4601-fe2a-b729253a7ac4"
      },
      "source": [
        "! wget http://yanran.li/files/ijcnlp_dailydialog.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-04 00:58:48--  http://yanran.li/files/ijcnlp_dailydialog.zip\n",
            "Resolving yanran.li (yanran.li)... 192.30.252.153, 192.30.252.154, 207.97.227.245\n",
            "Connecting to yanran.li (yanran.li)|192.30.252.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475921 (4.3M) [application/zip]\n",
            "Saving to: ‘ijcnlp_dailydialog.zip’\n",
            "\n",
            "ijcnlp_dailydialog. 100%[===================>]   4.27M  9.30MB/s    in 0.5s    \n",
            "\n",
            "2021-12-04 00:58:49 (9.30 MB/s) - ‘ijcnlp_dailydialog.zip’ saved [4475921/4475921]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paY28wsmztNJ"
      },
      "source": [
        "! mkdir data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJgy5mcZz2nu"
      },
      "source": [
        "! mkdir data/dialogdataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkdqNaGbz50w",
        "outputId": "1cc9be4f-73d1-4ec4-81da-15aa12793621"
      },
      "source": [
        "! unzip  ijcnlp_dailydialog.zip -d data/dialogdataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ijcnlp_dailydialog.zip\n",
            "   creating: data/dialogdataset/ijcnlp_dailydialog/\n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/.DS_Store  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/dialogues_act.txt  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/dialogues_emotion.txt  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/dialogues_text.txt  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/dialogues_topic.txt  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/readme.txt  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/test.zip  \n",
            " extracting: data/dialogdataset/ijcnlp_dailydialog/train.zip  \n",
            "  inflating: data/dialogdataset/ijcnlp_dailydialog/validation.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIFCN3cJp86"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqP3sM9ZdGRH",
        "outputId": "d3a58c19-b094-4819-badb-76b7f3d9aacf"
      },
      "source": [
        "corpus_name = \"dialogdataset/ijcnlp_dailydialog/\"\n",
        "corpus = os.path.join(\"data\", corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"dialogues_text.txt\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The kitchen stinks . __eou__ I'll throw out the garbage . __eou__\n",
            "\n",
            "So Dick , how about getting some coffee for tonight ? __eou__ Coffee ? I don ’ t honestly like that kind of stuff . __eou__ Come on , you can at least try a little , besides your cigarette . __eou__ What ’ s wrong with that ? Cigarette is the thing I go crazy for . __eou__ Not for me , Dick . __eou__\n",
            "\n",
            "Are things still going badly with your houseguest ? __eou__ Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . __eou__ Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law . __eou__ You ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed . __eou__\n",
            "\n",
            "Would you mind waiting a while ? __eou__ Well , how long will it be ? __eou__ I'm not sure . But I'll get a table ready as fast as I can . __eou__ OK . We'll wait . __eou__\n",
            "\n",
            "Are you going to the annual party ? I can give you a ride if you need one . __eou__ Thanks a lot . That's the favor I was going to ask you for . __eou__ The pleasure is mine . __eou__\n",
            "\n",
            "Isn ’ t he the best instructor ? I think he ’ s so hot . Wow ! I really feel energized , don ’ t you ? __eou__ I swear , I ’ m going to kill you for this . __eou__ What ’ s wrong ? Didn ’ t you think it was fun ? ! __eou__ Oh , yeah ! I had a blast ! I love sweating like a pig with a bunch of pot bellies who all smell bad . Sorry , I ’ m just not into this health kick . __eou__ Oh , no , get off it . It wasn ’ t such a killer class . You just have to get into it . Like they say , no pain , no gain . __eou__ I am wiped out . Thank you . __eou__ Look , next time get yourself some comfy shoes . You ’ re gonna come back again with me , aren ’ t you ? __eou__ Never ! But thank you for inviting me . __eou__ Come on . You ’ ll feel better after we hit the showers . __eou__\n",
            "\n",
            "Can I take your order now or do you still want to look at the menu ? __eou__ Well , I want a fillet steak , medium , but my little girl doesn't care for steak . Could she have something else instead ? __eou__ Certainly . How about spaghetti with clams and shrimps . __eou__ Sounds delicious . OK . She'll try that . __eou__\n",
            "\n",
            "Can you manage chopsticks ? __eou__ Why not ? See . __eou__ Good mastery . How do you like our Chinese food ? __eou__ Oh , great ! It's delicious . You see , I am already putting on weight . There is one thing I don't like however , MSG . __eou__ What's wrong with MSG ? It helps to bring out the taste of the food . __eou__ According to some studies it may cause cancer . __eou__ Oh , don't let that worry you . If that were true , China wouldn't have such a large population . __eou__ I just happen to have a question for you guys . Why do the Chinese cook the vegetables ? You see what I mean is that most vitamin are destroyed when heated . __eou__ I don't know exactly . It's a tradition . Maybe it's for sanitary reasons . __eou__\n",
            "\n",
            "I'm exhausted . __eou__ Okay , let's go home . __eou__\n",
            "\n",
            "Good evening . Welcome to Cherry's . Do you have a reservation ? __eou__ No , we don't . __eou__ How many of you , please ? __eou__ Six , including two kids . __eou__ I'm afraid all the big tables are taken . __eou__\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2nG7PWyJsK0"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvPJ1UEFROBo"
      },
      "source": [
        "# Splits each line of the file into a dictionary of fields\n",
        "def loadLines(fileName):\n",
        "    conversations = []\n",
        "    with open(fileName, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split(\"__eou__\")\n",
        "            conversations.append(values)\n",
        "    return conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations:\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[i].strip()\n",
        "            targetLine = conversation[i+1].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtjtdMl0dGRH",
        "outputId": "5e0238fe-b090-4213-818f-f5ed9abc0f7f"
      },
      "source": [
        "# Define path to new file\n",
        "datafile = os.path.join(corpus, \"formatted_dialogues_text.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "print(\"\\nLoading conversations...\")\n",
        "conversations = loadLines(os.path.join(corpus, \"dialogues_text.txt\"))\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Print a sample of lines\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "The kitchen stinks .\tI'll throw out the garbage .\n",
            "\n",
            "So Dick , how about getting some coffee for tonight ?\tCoffee ? I don ’ t honestly like that kind of stuff .\n",
            "\n",
            "Coffee ? I don ’ t honestly like that kind of stuff .\tCome on , you can at least try a little , besides your cigarette .\n",
            "\n",
            "Come on , you can at least try a little , besides your cigarette .\tWhat ’ s wrong with that ? Cigarette is the thing I go crazy for .\n",
            "\n",
            "What ’ s wrong with that ? Cigarette is the thing I go crazy for .\tNot for me , Dick .\n",
            "\n",
            "Are things still going badly with your houseguest ?\tGetting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .\n",
            "\n",
            "Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .\tLeo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .\n",
            "\n",
            "Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .\tYou ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed .\n",
            "\n",
            "Would you mind waiting a while ?\tWell , how long will it be ?\n",
            "\n",
            "Well , how long will it be ?\tI'm not sure . But I'll get a table ready as fast as I can .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYSD-dc4JvPk"
      },
      "source": [
        "## Vocabulary Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6UmpJpodGRH"
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyFq1oOzdGRI"
      },
      "source": [
        "# Dull set (from RL-chatbot)\n",
        "dull_responses = [\"I don't know what you're talking about.\", \"I don't know.\",\n",
        "            \"You don't know.\", \"You know what I mean.\", \"I know what you mean.\",\n",
        "            \"You know what I'm saying.\", \"You don't know anything.\"]\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InmB_sP8Jyfe"
      },
      "source": [
        "## Data Preprocessing( trimming, uppercase to lowercase, unicode to ascii)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8NMlkwfdGRI"
      },
      "source": [
        "MAX_LENGTH = 15  # Maximum sentence length to consider\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Read query/response pairs and return a voc object\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split into lines\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
        "def filterPair(p):\n",
        "    # Input sequences need to preserve the last word for EOS token\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Filter pairs using filterPair condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for d in dull_responses:\n",
        "        voc.addSentence(d)\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q03p23aWJ7a4"
      },
      "source": [
        "## Question Answer Pair Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4xGFEb1dGRI",
        "outputId": "5b0f3d32-3b94-413d-a62e-1314570b0dc5"
      },
      "source": [
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 89862 sentence pairs\n",
            "Trimmed to 43724 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 10129\n",
            "\n",
            "pairs:\n",
            "['the kitchen stinks .', 'i ll throw out the garbage .']\n",
            "['so dick how about getting some coffee for tonight ?', 'coffee ? i don t honestly like that kind of stuff .']\n",
            "['coffee ? i don t honestly like that kind of stuff .', 'come on you can at least try a little besides your cigarette .']\n",
            "['would you mind waiting a while ?', 'well how long will it be ?']\n",
            "['i swear i m going to kill you for this .', 'what s wrong ? didn t you think it was fun ? !']\n",
            "['never ! but thank you for inviting me .', 'come on . you ll feel better after we hit the showers .']\n",
            "['certainly . how about spaghetti with clams and shrimps .', 'sounds delicious . ok . she ll try that .']\n",
            "['can you manage chopsticks ?', 'why not ? see .']\n",
            "['why not ? see .', 'good mastery . how do you like our chinese food ?']\n",
            "['i m exhausted .', 'okay let s go home .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc1Gm2HedGRI",
        "outputId": "53731bf3-95d7-402b-dcfc-bde274a73f66"
      },
      "source": [
        "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 6160 / 10126 = 0.6083\n",
            "Trimmed from 43724 pairs to 38716, 0.8855 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO4JCKz8KA1s"
      },
      "source": [
        "## Mapping words to indexes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "debNVJWGdGRI",
        "outputId": "86177efa-2477-41bb-995f-fab8d2207170"
      },
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "  return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "  m = []\n",
        "  for i, seq in enumerate(l):\n",
        "      m.append([])\n",
        "      for token in seq:\n",
        "          if token == PAD_token:\n",
        "              m[i].append(0)\n",
        "          else:\n",
        "              m[i].append(1)\n",
        "  return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "  indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "  padList = zeroPadding(indexes_batch)\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "  indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "  max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "  padList = zeroPadding(indexes_batch)\n",
        "  mask = binaryMatrix(padList)\n",
        "  mask = torch.BoolTensor(mask)\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "  pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "  input_batch, output_batch = [], []\n",
        "  for pair in pair_batch:\n",
        "      input_batch.append(pair[0])\n",
        "      output_batch.append(pair[1])\n",
        "  inp, lengths = inputVar(input_batch, voc)\n",
        "  output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "  return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# Example for validation\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[  51,    9,  385,  484,   13],\n",
            "        [ 124, 1386,   73,   53,   57],\n",
            "        [  32, 1659,   20,   77,  291],\n",
            "        [1262,  361,   52,    5, 1362],\n",
            "        [ 110,  233,  159,  489,   81],\n",
            "        [ 202,   12,   10,  166,   54],\n",
            "        [  73,    6, 1766, 2097, 3125],\n",
            "        [  23,   62,   59,   10,   12],\n",
            "        [1421,   10, 4981,  976,    2],\n",
            "        [ 132,  456,   27,   27,    0],\n",
            "        [  12,   27,    2,    2,    0],\n",
            "        [   2,    2,    0,    0,    0]])\n",
            "lengths: tensor([12, 12, 11, 11,  9])\n",
            "target_variable: tensor([[ 237,   13,   54,   10,   13],\n",
            "        [  42,  559,   62, 4012,   57],\n",
            "        [ 236, 4781,   21, 1376,  535],\n",
            "        [  13,   83, 1681, 4013,    9],\n",
            "        [  53,   13,   12,   59,  278],\n",
            "        [ 995,   57,   77,  305,   18],\n",
            "        [   9,  593,  119,  976,  362],\n",
            "        [  23,  658,  557, 1355,   12],\n",
            "        [1967,  296,   59,   42,    2],\n",
            "        [  12,   12, 5640, 1025,    0],\n",
            "        [   2,    2,  148,  233,    0],\n",
            "        [   0,    0,   39,   12,    0],\n",
            "        [   0,    0,   12,    2,    0],\n",
            "        [   0,    0,    2,    0,    0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True, False],\n",
            "        [ True,  True,  True,  True, False],\n",
            "        [False, False,  True,  True, False],\n",
            "        [False, False,  True,  True, False],\n",
            "        [False, False,  True, False, False]])\n",
            "max_target_len: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OryEriFGKF77"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5bkMFk_dGRJ"
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG7yZx61KSKA"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thHA_KYVGAJ0"
      },
      "source": [
        "def train(input_variable, lengths, target_variable, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, criterion, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    target_length = target_variable.size()[0]\n",
        "\n",
        "    # Initialize variables                  \n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Teacher forcing: next input is current target\n",
        "           \n",
        "            # Calculate and accumulate loss\n",
        "            loss += criterion(decoder_output, target_variable[t]) #mask_loss\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            print_losses.append(loss)\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            loss += criterion(decoder_output, target_variable[t]) #mask_loss\n",
        "            print_losses.append(loss)\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_qjhBC7F7lp"
      },
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, criterion, batch_size, clip)\n",
        "        \n",
        "        print_loss_total += loss\n",
        "\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "\n",
        "\n",
        "        if iteration % save_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / save_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXQ3OYkgGFKN",
        "outputId": "0a8fa31d-e41f-46fe-a1d9-10c929755ceb"
      },
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 10000 # 4000\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    #checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHUVZM6IKZp9"
      },
      "source": [
        "## Running training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTYajq3VGIUs",
        "outputId": "fb507d45-3c3c-468d-cccc-98e2b72d6191"
      },
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 0.75\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 10000 #4000\n",
        "print_every = 10\n",
        "save_every = 1000\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n",
            "Iteration: 10; Percent complete: 0.1%; Average loss: 4.4179\n",
            "Iteration: 20; Percent complete: 0.2%; Average loss: 3.7143\n",
            "Iteration: 30; Percent complete: 0.3%; Average loss: 4.3745\n",
            "Iteration: 40; Percent complete: 0.4%; Average loss: 4.1496\n",
            "Iteration: 50; Percent complete: 0.5%; Average loss: 3.7071\n",
            "Iteration: 60; Percent complete: 0.6%; Average loss: 4.6551\n",
            "Iteration: 70; Percent complete: 0.7%; Average loss: 4.4893\n",
            "Iteration: 80; Percent complete: 0.8%; Average loss: 4.0902\n",
            "Iteration: 90; Percent complete: 0.9%; Average loss: 4.7003\n",
            "Iteration: 100; Percent complete: 1.0%; Average loss: 3.8241\n",
            "Iteration: 110; Percent complete: 1.1%; Average loss: 3.7433\n",
            "Iteration: 120; Percent complete: 1.2%; Average loss: 4.0257\n",
            "Iteration: 130; Percent complete: 1.3%; Average loss: 3.5401\n",
            "Iteration: 140; Percent complete: 1.4%; Average loss: 3.9490\n",
            "Iteration: 150; Percent complete: 1.5%; Average loss: 4.0540\n",
            "Iteration: 160; Percent complete: 1.6%; Average loss: 4.1987\n",
            "Iteration: 170; Percent complete: 1.7%; Average loss: 4.0254\n",
            "Iteration: 180; Percent complete: 1.8%; Average loss: 3.7999\n",
            "Iteration: 190; Percent complete: 1.9%; Average loss: 4.0757\n",
            "Iteration: 200; Percent complete: 2.0%; Average loss: 3.5116\n",
            "Iteration: 210; Percent complete: 2.1%; Average loss: 3.7916\n",
            "Iteration: 220; Percent complete: 2.2%; Average loss: 4.4568\n",
            "Iteration: 230; Percent complete: 2.3%; Average loss: 3.7044\n",
            "Iteration: 240; Percent complete: 2.4%; Average loss: 3.7742\n",
            "Iteration: 250; Percent complete: 2.5%; Average loss: 3.5915\n",
            "Iteration: 260; Percent complete: 2.6%; Average loss: 4.0192\n",
            "Iteration: 270; Percent complete: 2.7%; Average loss: 3.6928\n",
            "Iteration: 280; Percent complete: 2.8%; Average loss: 3.5080\n",
            "Iteration: 290; Percent complete: 2.9%; Average loss: 3.6946\n",
            "Iteration: 300; Percent complete: 3.0%; Average loss: 3.4503\n",
            "Iteration: 310; Percent complete: 3.1%; Average loss: 3.5836\n",
            "Iteration: 320; Percent complete: 3.2%; Average loss: 3.5021\n",
            "Iteration: 330; Percent complete: 3.3%; Average loss: 3.1724\n",
            "Iteration: 340; Percent complete: 3.4%; Average loss: 3.7120\n",
            "Iteration: 350; Percent complete: 3.5%; Average loss: 3.9525\n",
            "Iteration: 360; Percent complete: 3.6%; Average loss: 3.8839\n",
            "Iteration: 370; Percent complete: 3.7%; Average loss: 4.2956\n",
            "Iteration: 380; Percent complete: 3.8%; Average loss: 3.5577\n",
            "Iteration: 390; Percent complete: 3.9%; Average loss: 3.7145\n",
            "Iteration: 400; Percent complete: 4.0%; Average loss: 3.7262\n",
            "Iteration: 410; Percent complete: 4.1%; Average loss: 3.9559\n",
            "Iteration: 420; Percent complete: 4.2%; Average loss: 3.1592\n",
            "Iteration: 430; Percent complete: 4.3%; Average loss: 3.2675\n",
            "Iteration: 440; Percent complete: 4.4%; Average loss: 3.4749\n",
            "Iteration: 450; Percent complete: 4.5%; Average loss: 3.4669\n",
            "Iteration: 460; Percent complete: 4.6%; Average loss: 4.0489\n",
            "Iteration: 470; Percent complete: 4.7%; Average loss: 3.8085\n",
            "Iteration: 480; Percent complete: 4.8%; Average loss: 3.9778\n",
            "Iteration: 490; Percent complete: 4.9%; Average loss: 3.7534\n",
            "Iteration: 500; Percent complete: 5.0%; Average loss: 4.0188\n",
            "Iteration: 510; Percent complete: 5.1%; Average loss: 3.6190\n",
            "Iteration: 520; Percent complete: 5.2%; Average loss: 3.7054\n",
            "Iteration: 530; Percent complete: 5.3%; Average loss: 4.1211\n",
            "Iteration: 540; Percent complete: 5.4%; Average loss: 4.1094\n",
            "Iteration: 550; Percent complete: 5.5%; Average loss: 4.0045\n",
            "Iteration: 560; Percent complete: 5.6%; Average loss: 3.6423\n",
            "Iteration: 570; Percent complete: 5.7%; Average loss: 3.5818\n",
            "Iteration: 580; Percent complete: 5.8%; Average loss: 3.6187\n",
            "Iteration: 590; Percent complete: 5.9%; Average loss: 4.0229\n",
            "Iteration: 600; Percent complete: 6.0%; Average loss: 4.1318\n",
            "Iteration: 610; Percent complete: 6.1%; Average loss: 3.6778\n",
            "Iteration: 620; Percent complete: 6.2%; Average loss: 3.5668\n",
            "Iteration: 630; Percent complete: 6.3%; Average loss: 3.9145\n",
            "Iteration: 640; Percent complete: 6.4%; Average loss: 3.7952\n",
            "Iteration: 650; Percent complete: 6.5%; Average loss: 3.6350\n",
            "Iteration: 660; Percent complete: 6.6%; Average loss: 3.9703\n",
            "Iteration: 670; Percent complete: 6.7%; Average loss: 3.3788\n",
            "Iteration: 680; Percent complete: 6.8%; Average loss: 3.8024\n",
            "Iteration: 690; Percent complete: 6.9%; Average loss: 3.6625\n",
            "Iteration: 700; Percent complete: 7.0%; Average loss: 3.5485\n",
            "Iteration: 710; Percent complete: 7.1%; Average loss: 3.7146\n",
            "Iteration: 720; Percent complete: 7.2%; Average loss: 3.5046\n",
            "Iteration: 730; Percent complete: 7.3%; Average loss: 3.5255\n",
            "Iteration: 740; Percent complete: 7.4%; Average loss: 3.5115\n",
            "Iteration: 750; Percent complete: 7.5%; Average loss: 3.8385\n",
            "Iteration: 760; Percent complete: 7.6%; Average loss: 3.4684\n",
            "Iteration: 770; Percent complete: 7.7%; Average loss: 3.1643\n",
            "Iteration: 780; Percent complete: 7.8%; Average loss: 3.7482\n",
            "Iteration: 790; Percent complete: 7.9%; Average loss: 3.6715\n",
            "Iteration: 800; Percent complete: 8.0%; Average loss: 3.3309\n",
            "Iteration: 810; Percent complete: 8.1%; Average loss: 3.4890\n",
            "Iteration: 820; Percent complete: 8.2%; Average loss: 3.7419\n",
            "Iteration: 830; Percent complete: 8.3%; Average loss: 3.8003\n",
            "Iteration: 840; Percent complete: 8.4%; Average loss: 3.4821\n",
            "Iteration: 850; Percent complete: 8.5%; Average loss: 3.6082\n",
            "Iteration: 860; Percent complete: 8.6%; Average loss: 3.4926\n",
            "Iteration: 870; Percent complete: 8.7%; Average loss: 3.5281\n",
            "Iteration: 880; Percent complete: 8.8%; Average loss: 3.6464\n",
            "Iteration: 890; Percent complete: 8.9%; Average loss: 3.5573\n",
            "Iteration: 900; Percent complete: 9.0%; Average loss: 3.5064\n",
            "Iteration: 910; Percent complete: 9.1%; Average loss: 3.5936\n",
            "Iteration: 920; Percent complete: 9.2%; Average loss: 3.6302\n",
            "Iteration: 930; Percent complete: 9.3%; Average loss: 3.3657\n",
            "Iteration: 940; Percent complete: 9.4%; Average loss: 3.1622\n",
            "Iteration: 950; Percent complete: 9.5%; Average loss: 3.5020\n",
            "Iteration: 960; Percent complete: 9.6%; Average loss: 3.2294\n",
            "Iteration: 970; Percent complete: 9.7%; Average loss: 3.6424\n",
            "Iteration: 980; Percent complete: 9.8%; Average loss: 3.6399\n",
            "Iteration: 990; Percent complete: 9.9%; Average loss: 3.9191\n",
            "Iteration: 1000; Percent complete: 10.0%; Average loss: 3.5993\n",
            "Iteration: 1010; Percent complete: 10.1%; Average loss: 3.5395\n",
            "Iteration: 1020; Percent complete: 10.2%; Average loss: 3.5671\n",
            "Iteration: 1030; Percent complete: 10.3%; Average loss: 3.3425\n",
            "Iteration: 1040; Percent complete: 10.4%; Average loss: 3.9212\n",
            "Iteration: 1050; Percent complete: 10.5%; Average loss: 3.3848\n",
            "Iteration: 1060; Percent complete: 10.6%; Average loss: 3.7018\n",
            "Iteration: 1070; Percent complete: 10.7%; Average loss: 3.7716\n",
            "Iteration: 1080; Percent complete: 10.8%; Average loss: 3.3987\n",
            "Iteration: 1090; Percent complete: 10.9%; Average loss: 3.3719\n",
            "Iteration: 1100; Percent complete: 11.0%; Average loss: 3.7375\n",
            "Iteration: 1110; Percent complete: 11.1%; Average loss: 3.7853\n",
            "Iteration: 1120; Percent complete: 11.2%; Average loss: 3.5904\n",
            "Iteration: 1130; Percent complete: 11.3%; Average loss: 3.4987\n",
            "Iteration: 1140; Percent complete: 11.4%; Average loss: 3.2825\n",
            "Iteration: 1150; Percent complete: 11.5%; Average loss: 3.3986\n",
            "Iteration: 1160; Percent complete: 11.6%; Average loss: 3.7072\n",
            "Iteration: 1170; Percent complete: 11.7%; Average loss: 3.7499\n",
            "Iteration: 1180; Percent complete: 11.8%; Average loss: 3.5813\n",
            "Iteration: 1190; Percent complete: 11.9%; Average loss: 3.7618\n",
            "Iteration: 1200; Percent complete: 12.0%; Average loss: 3.4662\n",
            "Iteration: 1210; Percent complete: 12.1%; Average loss: 3.6237\n",
            "Iteration: 1220; Percent complete: 12.2%; Average loss: 3.4389\n",
            "Iteration: 1230; Percent complete: 12.3%; Average loss: 3.4725\n",
            "Iteration: 1240; Percent complete: 12.4%; Average loss: 3.7073\n",
            "Iteration: 1250; Percent complete: 12.5%; Average loss: 3.6392\n",
            "Iteration: 1260; Percent complete: 12.6%; Average loss: 3.5431\n",
            "Iteration: 1270; Percent complete: 12.7%; Average loss: 3.6089\n",
            "Iteration: 1280; Percent complete: 12.8%; Average loss: 3.5282\n",
            "Iteration: 1290; Percent complete: 12.9%; Average loss: 3.4316\n",
            "Iteration: 1300; Percent complete: 13.0%; Average loss: 3.4916\n",
            "Iteration: 1310; Percent complete: 13.1%; Average loss: 3.5286\n",
            "Iteration: 1320; Percent complete: 13.2%; Average loss: 3.5210\n",
            "Iteration: 1330; Percent complete: 13.3%; Average loss: 3.5092\n",
            "Iteration: 1340; Percent complete: 13.4%; Average loss: 3.5116\n",
            "Iteration: 1350; Percent complete: 13.5%; Average loss: 3.4263\n",
            "Iteration: 1360; Percent complete: 13.6%; Average loss: 3.7858\n",
            "Iteration: 1370; Percent complete: 13.7%; Average loss: 3.5265\n",
            "Iteration: 1380; Percent complete: 13.8%; Average loss: 3.4324\n",
            "Iteration: 1390; Percent complete: 13.9%; Average loss: 3.4310\n",
            "Iteration: 1400; Percent complete: 14.0%; Average loss: 3.5790\n",
            "Iteration: 1410; Percent complete: 14.1%; Average loss: 3.7215\n",
            "Iteration: 1420; Percent complete: 14.2%; Average loss: 3.3397\n",
            "Iteration: 1430; Percent complete: 14.3%; Average loss: 3.4758\n",
            "Iteration: 1440; Percent complete: 14.4%; Average loss: 3.7274\n",
            "Iteration: 1450; Percent complete: 14.5%; Average loss: 3.5977\n",
            "Iteration: 1460; Percent complete: 14.6%; Average loss: 3.5961\n",
            "Iteration: 1470; Percent complete: 14.7%; Average loss: 3.6504\n",
            "Iteration: 1480; Percent complete: 14.8%; Average loss: 3.4231\n",
            "Iteration: 1490; Percent complete: 14.9%; Average loss: 3.2763\n",
            "Iteration: 1500; Percent complete: 15.0%; Average loss: 3.3756\n",
            "Iteration: 1510; Percent complete: 15.1%; Average loss: 3.6668\n",
            "Iteration: 1520; Percent complete: 15.2%; Average loss: 3.6582\n",
            "Iteration: 1530; Percent complete: 15.3%; Average loss: 3.5329\n",
            "Iteration: 1540; Percent complete: 15.4%; Average loss: 3.4028\n",
            "Iteration: 1550; Percent complete: 15.5%; Average loss: 3.5098\n",
            "Iteration: 1560; Percent complete: 15.6%; Average loss: 3.2500\n",
            "Iteration: 1570; Percent complete: 15.7%; Average loss: 3.7749\n",
            "Iteration: 1580; Percent complete: 15.8%; Average loss: 3.2564\n",
            "Iteration: 1590; Percent complete: 15.9%; Average loss: 3.7848\n",
            "Iteration: 1600; Percent complete: 16.0%; Average loss: 3.4827\n",
            "Iteration: 1610; Percent complete: 16.1%; Average loss: 3.2080\n",
            "Iteration: 1620; Percent complete: 16.2%; Average loss: 3.4361\n",
            "Iteration: 1630; Percent complete: 16.3%; Average loss: 3.8380\n",
            "Iteration: 1640; Percent complete: 16.4%; Average loss: 3.3816\n",
            "Iteration: 1650; Percent complete: 16.5%; Average loss: 3.6237\n",
            "Iteration: 1660; Percent complete: 16.6%; Average loss: 3.1748\n",
            "Iteration: 1670; Percent complete: 16.7%; Average loss: 3.5016\n",
            "Iteration: 1680; Percent complete: 16.8%; Average loss: 3.1895\n",
            "Iteration: 1690; Percent complete: 16.9%; Average loss: 3.3848\n",
            "Iteration: 1700; Percent complete: 17.0%; Average loss: 3.4193\n",
            "Iteration: 1710; Percent complete: 17.1%; Average loss: 3.8233\n",
            "Iteration: 1720; Percent complete: 17.2%; Average loss: 3.4264\n",
            "Iteration: 1730; Percent complete: 17.3%; Average loss: 3.6361\n",
            "Iteration: 1740; Percent complete: 17.4%; Average loss: 3.3832\n",
            "Iteration: 1750; Percent complete: 17.5%; Average loss: 3.3827\n",
            "Iteration: 1760; Percent complete: 17.6%; Average loss: 3.6568\n",
            "Iteration: 1770; Percent complete: 17.7%; Average loss: 3.5320\n",
            "Iteration: 1780; Percent complete: 17.8%; Average loss: 3.6371\n",
            "Iteration: 1790; Percent complete: 17.9%; Average loss: 3.6883\n",
            "Iteration: 1800; Percent complete: 18.0%; Average loss: 3.3630\n",
            "Iteration: 1810; Percent complete: 18.1%; Average loss: 3.3928\n",
            "Iteration: 1820; Percent complete: 18.2%; Average loss: 3.5401\n",
            "Iteration: 1830; Percent complete: 18.3%; Average loss: 3.2475\n",
            "Iteration: 1840; Percent complete: 18.4%; Average loss: 3.4792\n",
            "Iteration: 1850; Percent complete: 18.5%; Average loss: 3.3157\n",
            "Iteration: 1860; Percent complete: 18.6%; Average loss: 3.2699\n",
            "Iteration: 1870; Percent complete: 18.7%; Average loss: 3.3522\n",
            "Iteration: 1880; Percent complete: 18.8%; Average loss: 3.6246\n",
            "Iteration: 1890; Percent complete: 18.9%; Average loss: 3.4478\n",
            "Iteration: 1900; Percent complete: 19.0%; Average loss: 3.5315\n",
            "Iteration: 1910; Percent complete: 19.1%; Average loss: 3.4420\n",
            "Iteration: 1920; Percent complete: 19.2%; Average loss: 3.3287\n",
            "Iteration: 1930; Percent complete: 19.3%; Average loss: 3.3769\n",
            "Iteration: 1940; Percent complete: 19.4%; Average loss: 3.3311\n",
            "Iteration: 1950; Percent complete: 19.5%; Average loss: 3.5638\n",
            "Iteration: 1960; Percent complete: 19.6%; Average loss: 3.5588\n",
            "Iteration: 1970; Percent complete: 19.7%; Average loss: 3.4995\n",
            "Iteration: 1980; Percent complete: 19.8%; Average loss: 3.7292\n",
            "Iteration: 1990; Percent complete: 19.9%; Average loss: 3.6232\n",
            "Iteration: 2000; Percent complete: 20.0%; Average loss: 3.7771\n",
            "Iteration: 2010; Percent complete: 20.1%; Average loss: 3.5499\n",
            "Iteration: 2020; Percent complete: 20.2%; Average loss: 3.2776\n",
            "Iteration: 2030; Percent complete: 20.3%; Average loss: 3.4415\n",
            "Iteration: 2040; Percent complete: 20.4%; Average loss: 3.4589\n",
            "Iteration: 2050; Percent complete: 20.5%; Average loss: 3.4093\n",
            "Iteration: 2060; Percent complete: 20.6%; Average loss: 3.5286\n",
            "Iteration: 2070; Percent complete: 20.7%; Average loss: 3.5248\n",
            "Iteration: 2080; Percent complete: 20.8%; Average loss: 3.3794\n",
            "Iteration: 2090; Percent complete: 20.9%; Average loss: 3.4195\n",
            "Iteration: 2100; Percent complete: 21.0%; Average loss: 3.3834\n",
            "Iteration: 2110; Percent complete: 21.1%; Average loss: 3.4847\n",
            "Iteration: 2120; Percent complete: 21.2%; Average loss: 3.3813\n",
            "Iteration: 2130; Percent complete: 21.3%; Average loss: 3.4836\n",
            "Iteration: 2140; Percent complete: 21.4%; Average loss: 3.4110\n",
            "Iteration: 2150; Percent complete: 21.5%; Average loss: 3.5007\n",
            "Iteration: 2160; Percent complete: 21.6%; Average loss: 3.3155\n",
            "Iteration: 2170; Percent complete: 21.7%; Average loss: 3.5985\n",
            "Iteration: 2180; Percent complete: 21.8%; Average loss: 3.5266\n",
            "Iteration: 2190; Percent complete: 21.9%; Average loss: 3.2659\n",
            "Iteration: 2200; Percent complete: 22.0%; Average loss: 3.6013\n",
            "Iteration: 2210; Percent complete: 22.1%; Average loss: 3.5333\n",
            "Iteration: 2220; Percent complete: 22.2%; Average loss: 3.3780\n",
            "Iteration: 2230; Percent complete: 22.3%; Average loss: 3.4284\n",
            "Iteration: 2240; Percent complete: 22.4%; Average loss: 3.5175\n",
            "Iteration: 2250; Percent complete: 22.5%; Average loss: 3.5196\n",
            "Iteration: 2260; Percent complete: 22.6%; Average loss: 3.3312\n",
            "Iteration: 2270; Percent complete: 22.7%; Average loss: 3.5405\n",
            "Iteration: 2280; Percent complete: 22.8%; Average loss: 3.3389\n",
            "Iteration: 2290; Percent complete: 22.9%; Average loss: 3.4185\n",
            "Iteration: 2300; Percent complete: 23.0%; Average loss: 3.4802\n",
            "Iteration: 2310; Percent complete: 23.1%; Average loss: 3.4742\n",
            "Iteration: 2320; Percent complete: 23.2%; Average loss: 3.3897\n",
            "Iteration: 2330; Percent complete: 23.3%; Average loss: 3.4312\n",
            "Iteration: 2340; Percent complete: 23.4%; Average loss: 3.4333\n",
            "Iteration: 2350; Percent complete: 23.5%; Average loss: 3.3214\n",
            "Iteration: 2360; Percent complete: 23.6%; Average loss: 3.7436\n",
            "Iteration: 2370; Percent complete: 23.7%; Average loss: 3.3009\n",
            "Iteration: 2380; Percent complete: 23.8%; Average loss: 3.1804\n",
            "Iteration: 2390; Percent complete: 23.9%; Average loss: 3.3620\n",
            "Iteration: 2400; Percent complete: 24.0%; Average loss: 3.2384\n",
            "Iteration: 2410; Percent complete: 24.1%; Average loss: 3.7001\n",
            "Iteration: 2420; Percent complete: 24.2%; Average loss: 3.4941\n",
            "Iteration: 2430; Percent complete: 24.3%; Average loss: 3.7104\n",
            "Iteration: 2440; Percent complete: 24.4%; Average loss: 3.4285\n",
            "Iteration: 2450; Percent complete: 24.5%; Average loss: 3.5222\n",
            "Iteration: 2460; Percent complete: 24.6%; Average loss: 3.4005\n",
            "Iteration: 2470; Percent complete: 24.7%; Average loss: 3.1972\n",
            "Iteration: 2480; Percent complete: 24.8%; Average loss: 3.5814\n",
            "Iteration: 2490; Percent complete: 24.9%; Average loss: 3.3979\n",
            "Iteration: 2500; Percent complete: 25.0%; Average loss: 3.8200\n",
            "Iteration: 2510; Percent complete: 25.1%; Average loss: 3.3019\n",
            "Iteration: 2520; Percent complete: 25.2%; Average loss: 3.6416\n",
            "Iteration: 2530; Percent complete: 25.3%; Average loss: 3.3798\n",
            "Iteration: 2540; Percent complete: 25.4%; Average loss: 3.6018\n",
            "Iteration: 2550; Percent complete: 25.5%; Average loss: 3.3928\n",
            "Iteration: 2560; Percent complete: 25.6%; Average loss: 3.4191\n",
            "Iteration: 2570; Percent complete: 25.7%; Average loss: 3.3281\n",
            "Iteration: 2580; Percent complete: 25.8%; Average loss: 3.6287\n",
            "Iteration: 2590; Percent complete: 25.9%; Average loss: 3.5792\n",
            "Iteration: 2600; Percent complete: 26.0%; Average loss: 3.3658\n",
            "Iteration: 2610; Percent complete: 26.1%; Average loss: 3.3539\n",
            "Iteration: 2620; Percent complete: 26.2%; Average loss: 3.3045\n",
            "Iteration: 2630; Percent complete: 26.3%; Average loss: 3.2486\n",
            "Iteration: 2640; Percent complete: 26.4%; Average loss: 3.3890\n",
            "Iteration: 2650; Percent complete: 26.5%; Average loss: 3.3487\n",
            "Iteration: 2660; Percent complete: 26.6%; Average loss: 3.2857\n",
            "Iteration: 2670; Percent complete: 26.7%; Average loss: 3.4010\n",
            "Iteration: 2680; Percent complete: 26.8%; Average loss: 3.4533\n",
            "Iteration: 2690; Percent complete: 26.9%; Average loss: 3.3165\n",
            "Iteration: 2700; Percent complete: 27.0%; Average loss: 3.3511\n",
            "Iteration: 2710; Percent complete: 27.1%; Average loss: 3.6649\n",
            "Iteration: 2720; Percent complete: 27.2%; Average loss: 3.6388\n",
            "Iteration: 2730; Percent complete: 27.3%; Average loss: 3.5542\n",
            "Iteration: 2740; Percent complete: 27.4%; Average loss: 3.4016\n",
            "Iteration: 2750; Percent complete: 27.5%; Average loss: 3.4172\n",
            "Iteration: 2760; Percent complete: 27.6%; Average loss: 3.3695\n",
            "Iteration: 2770; Percent complete: 27.7%; Average loss: 3.4756\n",
            "Iteration: 2780; Percent complete: 27.8%; Average loss: 3.5489\n",
            "Iteration: 2790; Percent complete: 27.9%; Average loss: 3.7728\n",
            "Iteration: 2800; Percent complete: 28.0%; Average loss: 3.4229\n",
            "Iteration: 2810; Percent complete: 28.1%; Average loss: 3.4585\n",
            "Iteration: 2820; Percent complete: 28.2%; Average loss: 3.7409\n",
            "Iteration: 2830; Percent complete: 28.3%; Average loss: 3.8333\n",
            "Iteration: 2840; Percent complete: 28.4%; Average loss: 3.6717\n",
            "Iteration: 2850; Percent complete: 28.5%; Average loss: 3.3423\n",
            "Iteration: 2860; Percent complete: 28.6%; Average loss: 3.4354\n",
            "Iteration: 2870; Percent complete: 28.7%; Average loss: 3.3515\n",
            "Iteration: 2880; Percent complete: 28.8%; Average loss: 3.2705\n",
            "Iteration: 2890; Percent complete: 28.9%; Average loss: 3.3650\n",
            "Iteration: 2900; Percent complete: 29.0%; Average loss: 3.2492\n",
            "Iteration: 2910; Percent complete: 29.1%; Average loss: 3.6867\n",
            "Iteration: 2920; Percent complete: 29.2%; Average loss: 3.5794\n",
            "Iteration: 2930; Percent complete: 29.3%; Average loss: 3.6951\n",
            "Iteration: 2940; Percent complete: 29.4%; Average loss: 3.7031\n",
            "Iteration: 2950; Percent complete: 29.5%; Average loss: 3.5413\n",
            "Iteration: 2960; Percent complete: 29.6%; Average loss: 3.7842\n",
            "Iteration: 2970; Percent complete: 29.7%; Average loss: 3.2829\n",
            "Iteration: 2980; Percent complete: 29.8%; Average loss: 3.4427\n",
            "Iteration: 2990; Percent complete: 29.9%; Average loss: 3.4631\n",
            "Iteration: 3000; Percent complete: 30.0%; Average loss: 3.6711\n",
            "Iteration: 3010; Percent complete: 30.1%; Average loss: 3.4534\n",
            "Iteration: 3020; Percent complete: 30.2%; Average loss: 3.5605\n",
            "Iteration: 3030; Percent complete: 30.3%; Average loss: 3.2346\n",
            "Iteration: 3040; Percent complete: 30.4%; Average loss: 3.5817\n",
            "Iteration: 3050; Percent complete: 30.5%; Average loss: 3.8238\n",
            "Iteration: 3060; Percent complete: 30.6%; Average loss: 3.6246\n",
            "Iteration: 3070; Percent complete: 30.7%; Average loss: 3.4247\n",
            "Iteration: 3080; Percent complete: 30.8%; Average loss: 3.2784\n",
            "Iteration: 3090; Percent complete: 30.9%; Average loss: 3.5599\n",
            "Iteration: 3100; Percent complete: 31.0%; Average loss: 3.5365\n",
            "Iteration: 3110; Percent complete: 31.1%; Average loss: 3.2834\n",
            "Iteration: 3120; Percent complete: 31.2%; Average loss: 3.5431\n",
            "Iteration: 3130; Percent complete: 31.3%; Average loss: 3.7455\n",
            "Iteration: 3140; Percent complete: 31.4%; Average loss: 3.5284\n",
            "Iteration: 3150; Percent complete: 31.5%; Average loss: 3.5797\n",
            "Iteration: 3160; Percent complete: 31.6%; Average loss: 3.6312\n",
            "Iteration: 3170; Percent complete: 31.7%; Average loss: 3.5382\n",
            "Iteration: 3180; Percent complete: 31.8%; Average loss: 3.5662\n",
            "Iteration: 3190; Percent complete: 31.9%; Average loss: 3.5222\n",
            "Iteration: 3200; Percent complete: 32.0%; Average loss: 3.5007\n",
            "Iteration: 3210; Percent complete: 32.1%; Average loss: 3.5800\n",
            "Iteration: 3220; Percent complete: 32.2%; Average loss: 3.3909\n",
            "Iteration: 3230; Percent complete: 32.3%; Average loss: 3.6430\n",
            "Iteration: 3240; Percent complete: 32.4%; Average loss: 3.6205\n",
            "Iteration: 3250; Percent complete: 32.5%; Average loss: 3.6342\n",
            "Iteration: 3260; Percent complete: 32.6%; Average loss: 3.6984\n",
            "Iteration: 3270; Percent complete: 32.7%; Average loss: 3.2960\n",
            "Iteration: 3280; Percent complete: 32.8%; Average loss: 3.2541\n",
            "Iteration: 3290; Percent complete: 32.9%; Average loss: 3.4035\n",
            "Iteration: 3300; Percent complete: 33.0%; Average loss: 3.4065\n",
            "Iteration: 3310; Percent complete: 33.1%; Average loss: 3.2417\n",
            "Iteration: 3320; Percent complete: 33.2%; Average loss: 3.4732\n",
            "Iteration: 3330; Percent complete: 33.3%; Average loss: 3.5680\n",
            "Iteration: 3340; Percent complete: 33.4%; Average loss: 3.6027\n",
            "Iteration: 3350; Percent complete: 33.5%; Average loss: 3.6844\n",
            "Iteration: 3360; Percent complete: 33.6%; Average loss: 3.5174\n",
            "Iteration: 3370; Percent complete: 33.7%; Average loss: 3.4112\n",
            "Iteration: 3380; Percent complete: 33.8%; Average loss: 3.2979\n",
            "Iteration: 3390; Percent complete: 33.9%; Average loss: 3.5010\n",
            "Iteration: 3400; Percent complete: 34.0%; Average loss: 3.3173\n",
            "Iteration: 3410; Percent complete: 34.1%; Average loss: 3.3494\n",
            "Iteration: 3420; Percent complete: 34.2%; Average loss: 3.6004\n",
            "Iteration: 3430; Percent complete: 34.3%; Average loss: 3.4779\n",
            "Iteration: 3440; Percent complete: 34.4%; Average loss: 3.6840\n",
            "Iteration: 3450; Percent complete: 34.5%; Average loss: 3.5155\n",
            "Iteration: 3460; Percent complete: 34.6%; Average loss: 3.4799\n",
            "Iteration: 3470; Percent complete: 34.7%; Average loss: 3.2803\n",
            "Iteration: 3480; Percent complete: 34.8%; Average loss: 3.5905\n",
            "Iteration: 3490; Percent complete: 34.9%; Average loss: 3.3100\n",
            "Iteration: 3500; Percent complete: 35.0%; Average loss: 3.3791\n",
            "Iteration: 3510; Percent complete: 35.1%; Average loss: 3.1689\n",
            "Iteration: 3520; Percent complete: 35.2%; Average loss: 3.3689\n",
            "Iteration: 3530; Percent complete: 35.3%; Average loss: 3.3363\n",
            "Iteration: 3540; Percent complete: 35.4%; Average loss: 3.3041\n",
            "Iteration: 3550; Percent complete: 35.5%; Average loss: 3.7270\n",
            "Iteration: 3560; Percent complete: 35.6%; Average loss: 3.5290\n",
            "Iteration: 3570; Percent complete: 35.7%; Average loss: 3.2586\n",
            "Iteration: 3580; Percent complete: 35.8%; Average loss: 3.5374\n",
            "Iteration: 3590; Percent complete: 35.9%; Average loss: 3.5893\n",
            "Iteration: 3600; Percent complete: 36.0%; Average loss: 3.4748\n",
            "Iteration: 3610; Percent complete: 36.1%; Average loss: 3.1742\n",
            "Iteration: 3620; Percent complete: 36.2%; Average loss: 3.4188\n",
            "Iteration: 3630; Percent complete: 36.3%; Average loss: 3.2898\n",
            "Iteration: 3640; Percent complete: 36.4%; Average loss: 3.4603\n",
            "Iteration: 3650; Percent complete: 36.5%; Average loss: 3.3698\n",
            "Iteration: 3660; Percent complete: 36.6%; Average loss: 3.6617\n",
            "Iteration: 3670; Percent complete: 36.7%; Average loss: 3.5875\n",
            "Iteration: 3680; Percent complete: 36.8%; Average loss: 3.4748\n",
            "Iteration: 3690; Percent complete: 36.9%; Average loss: 3.4542\n",
            "Iteration: 3700; Percent complete: 37.0%; Average loss: 3.4491\n",
            "Iteration: 3710; Percent complete: 37.1%; Average loss: 3.4679\n",
            "Iteration: 3720; Percent complete: 37.2%; Average loss: 3.3043\n",
            "Iteration: 3730; Percent complete: 37.3%; Average loss: 3.4825\n",
            "Iteration: 3740; Percent complete: 37.4%; Average loss: 3.2555\n",
            "Iteration: 3750; Percent complete: 37.5%; Average loss: 3.3952\n",
            "Iteration: 3760; Percent complete: 37.6%; Average loss: 3.5614\n",
            "Iteration: 3770; Percent complete: 37.7%; Average loss: 3.5006\n",
            "Iteration: 3780; Percent complete: 37.8%; Average loss: 3.5087\n",
            "Iteration: 3790; Percent complete: 37.9%; Average loss: 3.3276\n",
            "Iteration: 3800; Percent complete: 38.0%; Average loss: 3.3722\n",
            "Iteration: 3810; Percent complete: 38.1%; Average loss: 3.7540\n",
            "Iteration: 3820; Percent complete: 38.2%; Average loss: 3.3566\n",
            "Iteration: 3830; Percent complete: 38.3%; Average loss: 3.5965\n",
            "Iteration: 3840; Percent complete: 38.4%; Average loss: 3.5997\n",
            "Iteration: 3850; Percent complete: 38.5%; Average loss: 3.2723\n",
            "Iteration: 3860; Percent complete: 38.6%; Average loss: 3.5523\n",
            "Iteration: 3870; Percent complete: 38.7%; Average loss: 3.3893\n",
            "Iteration: 3880; Percent complete: 38.8%; Average loss: 3.3216\n",
            "Iteration: 3890; Percent complete: 38.9%; Average loss: 3.4982\n",
            "Iteration: 3900; Percent complete: 39.0%; Average loss: 3.2613\n",
            "Iteration: 3910; Percent complete: 39.1%; Average loss: 3.5371\n",
            "Iteration: 3920; Percent complete: 39.2%; Average loss: 3.4557\n",
            "Iteration: 3930; Percent complete: 39.3%; Average loss: 3.5142\n",
            "Iteration: 3940; Percent complete: 39.4%; Average loss: 3.3927\n",
            "Iteration: 3950; Percent complete: 39.5%; Average loss: 3.3223\n",
            "Iteration: 3960; Percent complete: 39.6%; Average loss: 3.3518\n",
            "Iteration: 3970; Percent complete: 39.7%; Average loss: 3.4209\n",
            "Iteration: 3980; Percent complete: 39.8%; Average loss: 3.4366\n",
            "Iteration: 3990; Percent complete: 39.9%; Average loss: 3.5241\n",
            "Iteration: 4000; Percent complete: 40.0%; Average loss: 3.2952\n",
            "Iteration: 4010; Percent complete: 40.1%; Average loss: 3.4830\n",
            "Iteration: 4020; Percent complete: 40.2%; Average loss: 3.2397\n",
            "Iteration: 4030; Percent complete: 40.3%; Average loss: 3.5488\n",
            "Iteration: 4040; Percent complete: 40.4%; Average loss: 3.3374\n",
            "Iteration: 4050; Percent complete: 40.5%; Average loss: 3.5264\n",
            "Iteration: 4060; Percent complete: 40.6%; Average loss: 3.4298\n",
            "Iteration: 4070; Percent complete: 40.7%; Average loss: 3.5157\n",
            "Iteration: 4080; Percent complete: 40.8%; Average loss: 3.3433\n",
            "Iteration: 4090; Percent complete: 40.9%; Average loss: 3.4099\n",
            "Iteration: 4100; Percent complete: 41.0%; Average loss: 3.2756\n",
            "Iteration: 4110; Percent complete: 41.1%; Average loss: 3.5319\n",
            "Iteration: 4120; Percent complete: 41.2%; Average loss: 3.2752\n",
            "Iteration: 4130; Percent complete: 41.3%; Average loss: 3.4148\n",
            "Iteration: 4140; Percent complete: 41.4%; Average loss: 3.3921\n",
            "Iteration: 4150; Percent complete: 41.5%; Average loss: 3.3750\n",
            "Iteration: 4160; Percent complete: 41.6%; Average loss: 3.3772\n",
            "Iteration: 4170; Percent complete: 41.7%; Average loss: 3.3123\n",
            "Iteration: 4180; Percent complete: 41.8%; Average loss: 3.2376\n",
            "Iteration: 4190; Percent complete: 41.9%; Average loss: 3.5984\n",
            "Iteration: 4200; Percent complete: 42.0%; Average loss: 3.4534\n",
            "Iteration: 4210; Percent complete: 42.1%; Average loss: 3.3150\n",
            "Iteration: 4220; Percent complete: 42.2%; Average loss: 3.3793\n",
            "Iteration: 4230; Percent complete: 42.3%; Average loss: 3.2660\n",
            "Iteration: 4240; Percent complete: 42.4%; Average loss: 3.2280\n",
            "Iteration: 4250; Percent complete: 42.5%; Average loss: 3.4799\n",
            "Iteration: 4260; Percent complete: 42.6%; Average loss: 3.3018\n",
            "Iteration: 4270; Percent complete: 42.7%; Average loss: 3.2594\n",
            "Iteration: 4280; Percent complete: 42.8%; Average loss: 3.5636\n",
            "Iteration: 4290; Percent complete: 42.9%; Average loss: 3.3677\n",
            "Iteration: 4300; Percent complete: 43.0%; Average loss: 3.3824\n",
            "Iteration: 4310; Percent complete: 43.1%; Average loss: 3.5060\n",
            "Iteration: 4320; Percent complete: 43.2%; Average loss: 3.4925\n",
            "Iteration: 4330; Percent complete: 43.3%; Average loss: 3.3537\n",
            "Iteration: 4340; Percent complete: 43.4%; Average loss: 3.6515\n",
            "Iteration: 4350; Percent complete: 43.5%; Average loss: 3.2823\n",
            "Iteration: 4360; Percent complete: 43.6%; Average loss: 3.4211\n",
            "Iteration: 4370; Percent complete: 43.7%; Average loss: 3.4033\n",
            "Iteration: 4380; Percent complete: 43.8%; Average loss: 3.4721\n",
            "Iteration: 4390; Percent complete: 43.9%; Average loss: 3.6019\n",
            "Iteration: 4400; Percent complete: 44.0%; Average loss: 3.3079\n",
            "Iteration: 4410; Percent complete: 44.1%; Average loss: 3.5474\n",
            "Iteration: 4420; Percent complete: 44.2%; Average loss: 3.2416\n",
            "Iteration: 4430; Percent complete: 44.3%; Average loss: 3.1822\n",
            "Iteration: 4440; Percent complete: 44.4%; Average loss: 3.3446\n",
            "Iteration: 4450; Percent complete: 44.5%; Average loss: 3.1736\n",
            "Iteration: 4460; Percent complete: 44.6%; Average loss: 3.4448\n",
            "Iteration: 4470; Percent complete: 44.7%; Average loss: 3.5733\n",
            "Iteration: 4480; Percent complete: 44.8%; Average loss: 3.3963\n",
            "Iteration: 4490; Percent complete: 44.9%; Average loss: 3.5639\n",
            "Iteration: 4500; Percent complete: 45.0%; Average loss: 3.3379\n",
            "Iteration: 4510; Percent complete: 45.1%; Average loss: 3.4580\n",
            "Iteration: 4520; Percent complete: 45.2%; Average loss: 3.3521\n",
            "Iteration: 4530; Percent complete: 45.3%; Average loss: 3.2734\n",
            "Iteration: 4540; Percent complete: 45.4%; Average loss: 3.2509\n",
            "Iteration: 4550; Percent complete: 45.5%; Average loss: 3.2953\n",
            "Iteration: 4560; Percent complete: 45.6%; Average loss: 3.4656\n",
            "Iteration: 4570; Percent complete: 45.7%; Average loss: 3.2903\n",
            "Iteration: 4580; Percent complete: 45.8%; Average loss: 3.1017\n",
            "Iteration: 4590; Percent complete: 45.9%; Average loss: 3.6425\n",
            "Iteration: 4600; Percent complete: 46.0%; Average loss: 3.5161\n",
            "Iteration: 4610; Percent complete: 46.1%; Average loss: 3.4420\n",
            "Iteration: 4620; Percent complete: 46.2%; Average loss: 3.4757\n",
            "Iteration: 4630; Percent complete: 46.3%; Average loss: 3.2378\n",
            "Iteration: 4640; Percent complete: 46.4%; Average loss: 3.2422\n",
            "Iteration: 4650; Percent complete: 46.5%; Average loss: 3.1858\n",
            "Iteration: 4660; Percent complete: 46.6%; Average loss: 3.4365\n",
            "Iteration: 4670; Percent complete: 46.7%; Average loss: 3.4574\n",
            "Iteration: 4680; Percent complete: 46.8%; Average loss: 3.3174\n",
            "Iteration: 4690; Percent complete: 46.9%; Average loss: 3.3699\n",
            "Iteration: 4700; Percent complete: 47.0%; Average loss: 3.2083\n",
            "Iteration: 4710; Percent complete: 47.1%; Average loss: 3.3190\n",
            "Iteration: 4720; Percent complete: 47.2%; Average loss: 3.3670\n",
            "Iteration: 4730; Percent complete: 47.3%; Average loss: 3.2124\n",
            "Iteration: 4740; Percent complete: 47.4%; Average loss: 3.4063\n",
            "Iteration: 4750; Percent complete: 47.5%; Average loss: 3.4793\n",
            "Iteration: 4760; Percent complete: 47.6%; Average loss: 3.2664\n",
            "Iteration: 4770; Percent complete: 47.7%; Average loss: 3.3956\n",
            "Iteration: 4780; Percent complete: 47.8%; Average loss: 3.6200\n",
            "Iteration: 4790; Percent complete: 47.9%; Average loss: 3.2540\n",
            "Iteration: 4800; Percent complete: 48.0%; Average loss: 3.3460\n",
            "Iteration: 4810; Percent complete: 48.1%; Average loss: 3.4062\n",
            "Iteration: 4820; Percent complete: 48.2%; Average loss: 3.4338\n",
            "Iteration: 4830; Percent complete: 48.3%; Average loss: 3.3435\n",
            "Iteration: 4840; Percent complete: 48.4%; Average loss: 3.2377\n",
            "Iteration: 4850; Percent complete: 48.5%; Average loss: 3.2682\n",
            "Iteration: 4860; Percent complete: 48.6%; Average loss: 3.5986\n",
            "Iteration: 4870; Percent complete: 48.7%; Average loss: 3.4761\n",
            "Iteration: 4880; Percent complete: 48.8%; Average loss: 3.2264\n",
            "Iteration: 4890; Percent complete: 48.9%; Average loss: 3.3576\n",
            "Iteration: 4900; Percent complete: 49.0%; Average loss: 3.6279\n",
            "Iteration: 4910; Percent complete: 49.1%; Average loss: 3.6006\n",
            "Iteration: 4920; Percent complete: 49.2%; Average loss: 3.3518\n",
            "Iteration: 4930; Percent complete: 49.3%; Average loss: 3.3546\n",
            "Iteration: 4940; Percent complete: 49.4%; Average loss: 3.3461\n",
            "Iteration: 4950; Percent complete: 49.5%; Average loss: 3.1601\n",
            "Iteration: 4960; Percent complete: 49.6%; Average loss: 3.3743\n",
            "Iteration: 4970; Percent complete: 49.7%; Average loss: 3.4185\n",
            "Iteration: 4980; Percent complete: 49.8%; Average loss: 3.4181\n",
            "Iteration: 4990; Percent complete: 49.9%; Average loss: 3.5437\n",
            "Iteration: 5000; Percent complete: 50.0%; Average loss: 3.3143\n",
            "Iteration: 5010; Percent complete: 50.1%; Average loss: 3.2444\n",
            "Iteration: 5020; Percent complete: 50.2%; Average loss: 3.3992\n",
            "Iteration: 5030; Percent complete: 50.3%; Average loss: 3.4349\n",
            "Iteration: 5040; Percent complete: 50.4%; Average loss: 3.1555\n",
            "Iteration: 5050; Percent complete: 50.5%; Average loss: 3.2830\n",
            "Iteration: 5060; Percent complete: 50.6%; Average loss: 3.4093\n",
            "Iteration: 5070; Percent complete: 50.7%; Average loss: 3.2622\n",
            "Iteration: 5080; Percent complete: 50.8%; Average loss: 3.5835\n",
            "Iteration: 5090; Percent complete: 50.9%; Average loss: 3.2181\n",
            "Iteration: 5100; Percent complete: 51.0%; Average loss: 3.6629\n",
            "Iteration: 5110; Percent complete: 51.1%; Average loss: 3.2745\n",
            "Iteration: 5120; Percent complete: 51.2%; Average loss: 3.3480\n",
            "Iteration: 5130; Percent complete: 51.3%; Average loss: 3.1278\n",
            "Iteration: 5140; Percent complete: 51.4%; Average loss: 3.3626\n",
            "Iteration: 5150; Percent complete: 51.5%; Average loss: 3.4194\n",
            "Iteration: 5160; Percent complete: 51.6%; Average loss: 3.3232\n",
            "Iteration: 5170; Percent complete: 51.7%; Average loss: 3.3831\n",
            "Iteration: 5180; Percent complete: 51.8%; Average loss: 3.4019\n",
            "Iteration: 5190; Percent complete: 51.9%; Average loss: 3.4003\n",
            "Iteration: 5200; Percent complete: 52.0%; Average loss: 3.5150\n",
            "Iteration: 5210; Percent complete: 52.1%; Average loss: 3.2291\n",
            "Iteration: 5220; Percent complete: 52.2%; Average loss: 3.4800\n",
            "Iteration: 5230; Percent complete: 52.3%; Average loss: 3.2115\n",
            "Iteration: 5240; Percent complete: 52.4%; Average loss: 3.3686\n",
            "Iteration: 5250; Percent complete: 52.5%; Average loss: 3.2039\n",
            "Iteration: 5260; Percent complete: 52.6%; Average loss: 3.3555\n",
            "Iteration: 5270; Percent complete: 52.7%; Average loss: 3.2405\n",
            "Iteration: 5280; Percent complete: 52.8%; Average loss: 3.2823\n",
            "Iteration: 5290; Percent complete: 52.9%; Average loss: 3.3281\n",
            "Iteration: 5300; Percent complete: 53.0%; Average loss: 3.3317\n",
            "Iteration: 5310; Percent complete: 53.1%; Average loss: 3.3946\n",
            "Iteration: 5320; Percent complete: 53.2%; Average loss: 3.3301\n",
            "Iteration: 5330; Percent complete: 53.3%; Average loss: 3.3748\n",
            "Iteration: 5340; Percent complete: 53.4%; Average loss: 3.1841\n",
            "Iteration: 5350; Percent complete: 53.5%; Average loss: 3.0963\n",
            "Iteration: 5360; Percent complete: 53.6%; Average loss: 3.4832\n",
            "Iteration: 5370; Percent complete: 53.7%; Average loss: 3.2391\n",
            "Iteration: 5380; Percent complete: 53.8%; Average loss: 3.3470\n",
            "Iteration: 5390; Percent complete: 53.9%; Average loss: 3.4423\n",
            "Iteration: 5400; Percent complete: 54.0%; Average loss: 3.4232\n",
            "Iteration: 5410; Percent complete: 54.1%; Average loss: 3.3825\n",
            "Iteration: 5420; Percent complete: 54.2%; Average loss: 3.2015\n",
            "Iteration: 5430; Percent complete: 54.3%; Average loss: 3.1391\n",
            "Iteration: 5440; Percent complete: 54.4%; Average loss: 3.2232\n",
            "Iteration: 5450; Percent complete: 54.5%; Average loss: 3.3479\n",
            "Iteration: 5460; Percent complete: 54.6%; Average loss: 3.3569\n",
            "Iteration: 5470; Percent complete: 54.7%; Average loss: 3.4364\n",
            "Iteration: 5480; Percent complete: 54.8%; Average loss: 3.3078\n",
            "Iteration: 5490; Percent complete: 54.9%; Average loss: 3.2625\n",
            "Iteration: 5500; Percent complete: 55.0%; Average loss: 3.3107\n",
            "Iteration: 5510; Percent complete: 55.1%; Average loss: 3.1986\n",
            "Iteration: 5520; Percent complete: 55.2%; Average loss: 3.4179\n",
            "Iteration: 5530; Percent complete: 55.3%; Average loss: 3.4393\n",
            "Iteration: 5540; Percent complete: 55.4%; Average loss: 3.2943\n",
            "Iteration: 5550; Percent complete: 55.5%; Average loss: 3.3885\n",
            "Iteration: 5560; Percent complete: 55.6%; Average loss: 3.2715\n",
            "Iteration: 5570; Percent complete: 55.7%; Average loss: 3.2966\n",
            "Iteration: 5580; Percent complete: 55.8%; Average loss: 3.3305\n",
            "Iteration: 5590; Percent complete: 55.9%; Average loss: 3.2524\n",
            "Iteration: 5600; Percent complete: 56.0%; Average loss: 3.2325\n",
            "Iteration: 5610; Percent complete: 56.1%; Average loss: 3.1797\n",
            "Iteration: 5620; Percent complete: 56.2%; Average loss: 3.3218\n",
            "Iteration: 5630; Percent complete: 56.3%; Average loss: 3.4273\n",
            "Iteration: 5640; Percent complete: 56.4%; Average loss: 3.3616\n",
            "Iteration: 5650; Percent complete: 56.5%; Average loss: 3.2773\n",
            "Iteration: 5660; Percent complete: 56.6%; Average loss: 3.2973\n",
            "Iteration: 5670; Percent complete: 56.7%; Average loss: 3.4532\n",
            "Iteration: 5680; Percent complete: 56.8%; Average loss: 3.3381\n",
            "Iteration: 5690; Percent complete: 56.9%; Average loss: 3.4916\n",
            "Iteration: 5700; Percent complete: 57.0%; Average loss: 3.2751\n",
            "Iteration: 5710; Percent complete: 57.1%; Average loss: 3.2990\n",
            "Iteration: 5720; Percent complete: 57.2%; Average loss: 3.2077\n",
            "Iteration: 5730; Percent complete: 57.3%; Average loss: 3.5168\n",
            "Iteration: 5740; Percent complete: 57.4%; Average loss: 3.2995\n",
            "Iteration: 5750; Percent complete: 57.5%; Average loss: 3.4317\n",
            "Iteration: 5760; Percent complete: 57.6%; Average loss: 3.4246\n",
            "Iteration: 5770; Percent complete: 57.7%; Average loss: 3.4444\n",
            "Iteration: 5780; Percent complete: 57.8%; Average loss: 3.5276\n",
            "Iteration: 5790; Percent complete: 57.9%; Average loss: 3.5170\n",
            "Iteration: 5800; Percent complete: 58.0%; Average loss: 3.1989\n",
            "Iteration: 5810; Percent complete: 58.1%; Average loss: 3.3155\n",
            "Iteration: 5820; Percent complete: 58.2%; Average loss: 3.2207\n",
            "Iteration: 5830; Percent complete: 58.3%; Average loss: 3.3870\n",
            "Iteration: 5840; Percent complete: 58.4%; Average loss: 3.2653\n",
            "Iteration: 5850; Percent complete: 58.5%; Average loss: 3.4262\n",
            "Iteration: 5860; Percent complete: 58.6%; Average loss: 3.2314\n",
            "Iteration: 5870; Percent complete: 58.7%; Average loss: 3.3515\n",
            "Iteration: 5880; Percent complete: 58.8%; Average loss: 3.3447\n",
            "Iteration: 5890; Percent complete: 58.9%; Average loss: 3.4183\n",
            "Iteration: 5900; Percent complete: 59.0%; Average loss: 3.3353\n",
            "Iteration: 5910; Percent complete: 59.1%; Average loss: 3.3089\n",
            "Iteration: 5920; Percent complete: 59.2%; Average loss: 3.2802\n",
            "Iteration: 5930; Percent complete: 59.3%; Average loss: 3.2019\n",
            "Iteration: 5940; Percent complete: 59.4%; Average loss: 3.2160\n",
            "Iteration: 5950; Percent complete: 59.5%; Average loss: 3.3911\n",
            "Iteration: 5960; Percent complete: 59.6%; Average loss: 3.2722\n",
            "Iteration: 5970; Percent complete: 59.7%; Average loss: 3.3189\n",
            "Iteration: 5980; Percent complete: 59.8%; Average loss: 3.1688\n",
            "Iteration: 5990; Percent complete: 59.9%; Average loss: 3.3999\n",
            "Iteration: 6000; Percent complete: 60.0%; Average loss: 3.4146\n",
            "Iteration: 6010; Percent complete: 60.1%; Average loss: 3.1528\n",
            "Iteration: 6020; Percent complete: 60.2%; Average loss: 3.3830\n",
            "Iteration: 6030; Percent complete: 60.3%; Average loss: 3.4380\n",
            "Iteration: 6040; Percent complete: 60.4%; Average loss: 3.3658\n",
            "Iteration: 6050; Percent complete: 60.5%; Average loss: 3.4081\n",
            "Iteration: 6060; Percent complete: 60.6%; Average loss: 3.3588\n",
            "Iteration: 6070; Percent complete: 60.7%; Average loss: 3.3669\n",
            "Iteration: 6080; Percent complete: 60.8%; Average loss: 3.3997\n",
            "Iteration: 6090; Percent complete: 60.9%; Average loss: 3.1661\n",
            "Iteration: 6100; Percent complete: 61.0%; Average loss: 3.2438\n",
            "Iteration: 6110; Percent complete: 61.1%; Average loss: 3.1659\n",
            "Iteration: 6120; Percent complete: 61.2%; Average loss: 3.3922\n",
            "Iteration: 6130; Percent complete: 61.3%; Average loss: 3.5217\n",
            "Iteration: 6140; Percent complete: 61.4%; Average loss: 3.3146\n",
            "Iteration: 6150; Percent complete: 61.5%; Average loss: 3.3775\n",
            "Iteration: 6160; Percent complete: 61.6%; Average loss: 3.2950\n",
            "Iteration: 6170; Percent complete: 61.7%; Average loss: 3.1606\n",
            "Iteration: 6180; Percent complete: 61.8%; Average loss: 3.4194\n",
            "Iteration: 6190; Percent complete: 61.9%; Average loss: 3.4735\n",
            "Iteration: 6200; Percent complete: 62.0%; Average loss: 3.1711\n",
            "Iteration: 6210; Percent complete: 62.1%; Average loss: 3.2839\n",
            "Iteration: 6220; Percent complete: 62.2%; Average loss: 3.5578\n",
            "Iteration: 6230; Percent complete: 62.3%; Average loss: 3.2434\n",
            "Iteration: 6240; Percent complete: 62.4%; Average loss: 3.4378\n",
            "Iteration: 6250; Percent complete: 62.5%; Average loss: 3.3447\n",
            "Iteration: 6260; Percent complete: 62.6%; Average loss: 3.4541\n",
            "Iteration: 6270; Percent complete: 62.7%; Average loss: 3.4521\n",
            "Iteration: 6280; Percent complete: 62.8%; Average loss: 3.3404\n",
            "Iteration: 6290; Percent complete: 62.9%; Average loss: 3.2497\n",
            "Iteration: 6300; Percent complete: 63.0%; Average loss: 3.3271\n",
            "Iteration: 6310; Percent complete: 63.1%; Average loss: 3.3176\n",
            "Iteration: 6320; Percent complete: 63.2%; Average loss: 3.2079\n",
            "Iteration: 6330; Percent complete: 63.3%; Average loss: 3.2989\n",
            "Iteration: 6340; Percent complete: 63.4%; Average loss: 3.3598\n",
            "Iteration: 6350; Percent complete: 63.5%; Average loss: 3.1665\n",
            "Iteration: 6360; Percent complete: 63.6%; Average loss: 3.2552\n",
            "Iteration: 6370; Percent complete: 63.7%; Average loss: 3.5004\n",
            "Iteration: 6380; Percent complete: 63.8%; Average loss: 3.2424\n",
            "Iteration: 6390; Percent complete: 63.9%; Average loss: 3.2613\n",
            "Iteration: 6400; Percent complete: 64.0%; Average loss: 3.1369\n",
            "Iteration: 6410; Percent complete: 64.1%; Average loss: 3.5311\n",
            "Iteration: 6420; Percent complete: 64.2%; Average loss: 3.2031\n",
            "Iteration: 6430; Percent complete: 64.3%; Average loss: 3.3391\n",
            "Iteration: 6440; Percent complete: 64.4%; Average loss: 3.2853\n",
            "Iteration: 6450; Percent complete: 64.5%; Average loss: 3.2749\n",
            "Iteration: 6460; Percent complete: 64.6%; Average loss: 3.2386\n",
            "Iteration: 6470; Percent complete: 64.7%; Average loss: 3.2999\n",
            "Iteration: 6480; Percent complete: 64.8%; Average loss: 3.3885\n",
            "Iteration: 6490; Percent complete: 64.9%; Average loss: 3.5132\n",
            "Iteration: 6500; Percent complete: 65.0%; Average loss: 3.4118\n",
            "Iteration: 6510; Percent complete: 65.1%; Average loss: 3.2979\n",
            "Iteration: 6520; Percent complete: 65.2%; Average loss: 3.1676\n",
            "Iteration: 6530; Percent complete: 65.3%; Average loss: 3.3757\n",
            "Iteration: 6540; Percent complete: 65.4%; Average loss: 3.3757\n",
            "Iteration: 6550; Percent complete: 65.5%; Average loss: 3.3532\n",
            "Iteration: 6560; Percent complete: 65.6%; Average loss: 3.5126\n",
            "Iteration: 6570; Percent complete: 65.7%; Average loss: 3.3489\n",
            "Iteration: 6580; Percent complete: 65.8%; Average loss: 3.2420\n",
            "Iteration: 6590; Percent complete: 65.9%; Average loss: 3.3160\n",
            "Iteration: 6600; Percent complete: 66.0%; Average loss: 3.2828\n",
            "Iteration: 6610; Percent complete: 66.1%; Average loss: 3.3533\n",
            "Iteration: 6620; Percent complete: 66.2%; Average loss: 3.4316\n",
            "Iteration: 6630; Percent complete: 66.3%; Average loss: 3.3746\n",
            "Iteration: 6640; Percent complete: 66.4%; Average loss: 3.2552\n",
            "Iteration: 6650; Percent complete: 66.5%; Average loss: 3.2341\n",
            "Iteration: 6660; Percent complete: 66.6%; Average loss: 3.0677\n",
            "Iteration: 6670; Percent complete: 66.7%; Average loss: 3.3982\n",
            "Iteration: 6680; Percent complete: 66.8%; Average loss: 3.2981\n",
            "Iteration: 6690; Percent complete: 66.9%; Average loss: 3.3234\n",
            "Iteration: 6700; Percent complete: 67.0%; Average loss: 3.2653\n",
            "Iteration: 6710; Percent complete: 67.1%; Average loss: 3.2600\n",
            "Iteration: 6720; Percent complete: 67.2%; Average loss: 3.4441\n",
            "Iteration: 6730; Percent complete: 67.3%; Average loss: 3.3647\n",
            "Iteration: 6740; Percent complete: 67.4%; Average loss: 3.3655\n",
            "Iteration: 6750; Percent complete: 67.5%; Average loss: 3.3830\n",
            "Iteration: 6760; Percent complete: 67.6%; Average loss: 3.3094\n",
            "Iteration: 6770; Percent complete: 67.7%; Average loss: 3.4146\n",
            "Iteration: 6780; Percent complete: 67.8%; Average loss: 3.1366\n",
            "Iteration: 6790; Percent complete: 67.9%; Average loss: 3.2492\n",
            "Iteration: 6800; Percent complete: 68.0%; Average loss: 3.1616\n",
            "Iteration: 6810; Percent complete: 68.1%; Average loss: 3.1502\n",
            "Iteration: 6820; Percent complete: 68.2%; Average loss: 3.2971\n",
            "Iteration: 6830; Percent complete: 68.3%; Average loss: 3.2408\n",
            "Iteration: 6840; Percent complete: 68.4%; Average loss: 3.2584\n",
            "Iteration: 6850; Percent complete: 68.5%; Average loss: 3.4236\n",
            "Iteration: 6860; Percent complete: 68.6%; Average loss: 3.3339\n",
            "Iteration: 6870; Percent complete: 68.7%; Average loss: 3.2584\n",
            "Iteration: 6880; Percent complete: 68.8%; Average loss: 3.2286\n",
            "Iteration: 6890; Percent complete: 68.9%; Average loss: 3.2901\n",
            "Iteration: 6900; Percent complete: 69.0%; Average loss: 3.3023\n",
            "Iteration: 6910; Percent complete: 69.1%; Average loss: 3.2311\n",
            "Iteration: 6920; Percent complete: 69.2%; Average loss: 3.2817\n",
            "Iteration: 6930; Percent complete: 69.3%; Average loss: 3.1274\n",
            "Iteration: 6940; Percent complete: 69.4%; Average loss: 3.4716\n",
            "Iteration: 6950; Percent complete: 69.5%; Average loss: 3.2067\n",
            "Iteration: 6960; Percent complete: 69.6%; Average loss: 3.2752\n",
            "Iteration: 6970; Percent complete: 69.7%; Average loss: 3.3569\n",
            "Iteration: 6980; Percent complete: 69.8%; Average loss: 3.4351\n",
            "Iteration: 6990; Percent complete: 69.9%; Average loss: 3.4063\n",
            "Iteration: 7000; Percent complete: 70.0%; Average loss: 3.2927\n",
            "Iteration: 7010; Percent complete: 70.1%; Average loss: 3.2699\n",
            "Iteration: 7020; Percent complete: 70.2%; Average loss: 3.2326\n",
            "Iteration: 7030; Percent complete: 70.3%; Average loss: 3.2970\n",
            "Iteration: 7040; Percent complete: 70.4%; Average loss: 3.1641\n",
            "Iteration: 7050; Percent complete: 70.5%; Average loss: 3.2070\n",
            "Iteration: 7060; Percent complete: 70.6%; Average loss: 3.2870\n",
            "Iteration: 7070; Percent complete: 70.7%; Average loss: 3.0956\n",
            "Iteration: 7080; Percent complete: 70.8%; Average loss: 3.4425\n",
            "Iteration: 7090; Percent complete: 70.9%; Average loss: 3.3340\n",
            "Iteration: 7100; Percent complete: 71.0%; Average loss: 3.3671\n",
            "Iteration: 7110; Percent complete: 71.1%; Average loss: 3.2150\n",
            "Iteration: 7120; Percent complete: 71.2%; Average loss: 3.1426\n",
            "Iteration: 7130; Percent complete: 71.3%; Average loss: 3.2404\n",
            "Iteration: 7140; Percent complete: 71.4%; Average loss: 3.4532\n",
            "Iteration: 7150; Percent complete: 71.5%; Average loss: 3.0858\n",
            "Iteration: 7160; Percent complete: 71.6%; Average loss: 3.3091\n",
            "Iteration: 7170; Percent complete: 71.7%; Average loss: 3.3905\n",
            "Iteration: 7180; Percent complete: 71.8%; Average loss: 3.2797\n",
            "Iteration: 7190; Percent complete: 71.9%; Average loss: 3.2398\n",
            "Iteration: 7200; Percent complete: 72.0%; Average loss: 3.4581\n",
            "Iteration: 7210; Percent complete: 72.1%; Average loss: 3.3462\n",
            "Iteration: 7220; Percent complete: 72.2%; Average loss: 3.2982\n",
            "Iteration: 7230; Percent complete: 72.3%; Average loss: 3.3067\n",
            "Iteration: 7240; Percent complete: 72.4%; Average loss: 3.1523\n",
            "Iteration: 7250; Percent complete: 72.5%; Average loss: 3.3812\n",
            "Iteration: 7260; Percent complete: 72.6%; Average loss: 3.2581\n",
            "Iteration: 7270; Percent complete: 72.7%; Average loss: 3.1620\n",
            "Iteration: 7280; Percent complete: 72.8%; Average loss: 3.3715\n",
            "Iteration: 7290; Percent complete: 72.9%; Average loss: 3.3786\n",
            "Iteration: 7300; Percent complete: 73.0%; Average loss: 3.2869\n",
            "Iteration: 7310; Percent complete: 73.1%; Average loss: 3.3452\n",
            "Iteration: 7320; Percent complete: 73.2%; Average loss: 3.2330\n",
            "Iteration: 7330; Percent complete: 73.3%; Average loss: 3.3089\n",
            "Iteration: 7340; Percent complete: 73.4%; Average loss: 3.2900\n",
            "Iteration: 7350; Percent complete: 73.5%; Average loss: 3.1847\n",
            "Iteration: 7360; Percent complete: 73.6%; Average loss: 3.2662\n",
            "Iteration: 7370; Percent complete: 73.7%; Average loss: 3.2107\n",
            "Iteration: 7380; Percent complete: 73.8%; Average loss: 3.2269\n",
            "Iteration: 7390; Percent complete: 73.9%; Average loss: 3.3262\n",
            "Iteration: 7400; Percent complete: 74.0%; Average loss: 3.2610\n",
            "Iteration: 7410; Percent complete: 74.1%; Average loss: 3.3055\n",
            "Iteration: 7420; Percent complete: 74.2%; Average loss: 3.3745\n",
            "Iteration: 7430; Percent complete: 74.3%; Average loss: 3.2374\n",
            "Iteration: 7440; Percent complete: 74.4%; Average loss: 3.2731\n",
            "Iteration: 7450; Percent complete: 74.5%; Average loss: 3.1678\n",
            "Iteration: 7460; Percent complete: 74.6%; Average loss: 3.1107\n",
            "Iteration: 7470; Percent complete: 74.7%; Average loss: 3.4293\n",
            "Iteration: 7480; Percent complete: 74.8%; Average loss: 3.3452\n",
            "Iteration: 7490; Percent complete: 74.9%; Average loss: 3.1842\n",
            "Iteration: 7500; Percent complete: 75.0%; Average loss: 3.3542\n",
            "Iteration: 7510; Percent complete: 75.1%; Average loss: 3.4576\n",
            "Iteration: 7520; Percent complete: 75.2%; Average loss: 3.2526\n",
            "Iteration: 7530; Percent complete: 75.3%; Average loss: 3.3402\n",
            "Iteration: 7540; Percent complete: 75.4%; Average loss: 3.3865\n",
            "Iteration: 7550; Percent complete: 75.5%; Average loss: 3.3379\n",
            "Iteration: 7560; Percent complete: 75.6%; Average loss: 3.3614\n",
            "Iteration: 7570; Percent complete: 75.7%; Average loss: 3.4344\n",
            "Iteration: 7580; Percent complete: 75.8%; Average loss: 3.2329\n",
            "Iteration: 7590; Percent complete: 75.9%; Average loss: 3.2212\n",
            "Iteration: 7600; Percent complete: 76.0%; Average loss: 3.2521\n",
            "Iteration: 7610; Percent complete: 76.1%; Average loss: 3.1627\n",
            "Iteration: 7620; Percent complete: 76.2%; Average loss: 3.4514\n",
            "Iteration: 7630; Percent complete: 76.3%; Average loss: 3.3970\n",
            "Iteration: 7640; Percent complete: 76.4%; Average loss: 3.1349\n",
            "Iteration: 7650; Percent complete: 76.5%; Average loss: 3.3128\n",
            "Iteration: 7660; Percent complete: 76.6%; Average loss: 3.1936\n",
            "Iteration: 7670; Percent complete: 76.7%; Average loss: 3.2521\n",
            "Iteration: 7680; Percent complete: 76.8%; Average loss: 3.2665\n",
            "Iteration: 7690; Percent complete: 76.9%; Average loss: 3.3912\n",
            "Iteration: 7700; Percent complete: 77.0%; Average loss: 3.2629\n",
            "Iteration: 7710; Percent complete: 77.1%; Average loss: 3.2736\n",
            "Iteration: 7720; Percent complete: 77.2%; Average loss: 3.4824\n",
            "Iteration: 7730; Percent complete: 77.3%; Average loss: 3.2229\n",
            "Iteration: 7740; Percent complete: 77.4%; Average loss: 3.2284\n",
            "Iteration: 7750; Percent complete: 77.5%; Average loss: 3.4793\n",
            "Iteration: 7760; Percent complete: 77.6%; Average loss: 3.1081\n",
            "Iteration: 7770; Percent complete: 77.7%; Average loss: 3.2479\n",
            "Iteration: 7780; Percent complete: 77.8%; Average loss: 3.1967\n",
            "Iteration: 7790; Percent complete: 77.9%; Average loss: 3.3995\n",
            "Iteration: 7800; Percent complete: 78.0%; Average loss: 3.1478\n",
            "Iteration: 7810; Percent complete: 78.1%; Average loss: 3.2633\n",
            "Iteration: 7820; Percent complete: 78.2%; Average loss: 3.1097\n",
            "Iteration: 7830; Percent complete: 78.3%; Average loss: 3.3473\n",
            "Iteration: 7840; Percent complete: 78.4%; Average loss: 3.2752\n",
            "Iteration: 7850; Percent complete: 78.5%; Average loss: 3.2787\n",
            "Iteration: 7860; Percent complete: 78.6%; Average loss: 3.3292\n",
            "Iteration: 7870; Percent complete: 78.7%; Average loss: 3.1142\n",
            "Iteration: 7880; Percent complete: 78.8%; Average loss: 3.2687\n",
            "Iteration: 7890; Percent complete: 78.9%; Average loss: 3.2895\n",
            "Iteration: 7900; Percent complete: 79.0%; Average loss: 3.1424\n",
            "Iteration: 7910; Percent complete: 79.1%; Average loss: 3.2165\n",
            "Iteration: 7920; Percent complete: 79.2%; Average loss: 3.2664\n",
            "Iteration: 7930; Percent complete: 79.3%; Average loss: 3.3160\n",
            "Iteration: 7940; Percent complete: 79.4%; Average loss: 3.1544\n",
            "Iteration: 7950; Percent complete: 79.5%; Average loss: 3.3046\n",
            "Iteration: 7960; Percent complete: 79.6%; Average loss: 3.4112\n",
            "Iteration: 7970; Percent complete: 79.7%; Average loss: 3.1023\n",
            "Iteration: 7980; Percent complete: 79.8%; Average loss: 3.2936\n",
            "Iteration: 7990; Percent complete: 79.9%; Average loss: 3.2121\n",
            "Iteration: 8000; Percent complete: 80.0%; Average loss: 3.3147\n",
            "Iteration: 8010; Percent complete: 80.1%; Average loss: 3.1597\n",
            "Iteration: 8020; Percent complete: 80.2%; Average loss: 3.0940\n",
            "Iteration: 8030; Percent complete: 80.3%; Average loss: 3.1945\n",
            "Iteration: 8040; Percent complete: 80.4%; Average loss: 3.3071\n",
            "Iteration: 8050; Percent complete: 80.5%; Average loss: 3.2657\n",
            "Iteration: 8060; Percent complete: 80.6%; Average loss: 3.3295\n",
            "Iteration: 8070; Percent complete: 80.7%; Average loss: 3.2882\n",
            "Iteration: 8080; Percent complete: 80.8%; Average loss: 3.1704\n",
            "Iteration: 8090; Percent complete: 80.9%; Average loss: 3.3240\n",
            "Iteration: 8100; Percent complete: 81.0%; Average loss: 3.0715\n",
            "Iteration: 8110; Percent complete: 81.1%; Average loss: 3.3674\n",
            "Iteration: 8120; Percent complete: 81.2%; Average loss: 3.2539\n",
            "Iteration: 8130; Percent complete: 81.3%; Average loss: 3.3270\n",
            "Iteration: 8140; Percent complete: 81.4%; Average loss: 3.1886\n",
            "Iteration: 8150; Percent complete: 81.5%; Average loss: 3.1885\n",
            "Iteration: 8160; Percent complete: 81.6%; Average loss: 3.2387\n",
            "Iteration: 8170; Percent complete: 81.7%; Average loss: 3.1937\n",
            "Iteration: 8180; Percent complete: 81.8%; Average loss: 3.3041\n",
            "Iteration: 8190; Percent complete: 81.9%; Average loss: 3.3956\n",
            "Iteration: 8200; Percent complete: 82.0%; Average loss: 3.2701\n",
            "Iteration: 8210; Percent complete: 82.1%; Average loss: 3.3539\n",
            "Iteration: 8220; Percent complete: 82.2%; Average loss: 3.3054\n",
            "Iteration: 8230; Percent complete: 82.3%; Average loss: 3.2218\n",
            "Iteration: 8240; Percent complete: 82.4%; Average loss: 3.4504\n",
            "Iteration: 8250; Percent complete: 82.5%; Average loss: 3.2547\n",
            "Iteration: 8260; Percent complete: 82.6%; Average loss: 3.2183\n",
            "Iteration: 8270; Percent complete: 82.7%; Average loss: 3.3432\n",
            "Iteration: 8280; Percent complete: 82.8%; Average loss: 3.2997\n",
            "Iteration: 8290; Percent complete: 82.9%; Average loss: 3.3588\n",
            "Iteration: 8300; Percent complete: 83.0%; Average loss: 3.1774\n",
            "Iteration: 8310; Percent complete: 83.1%; Average loss: 3.1961\n",
            "Iteration: 8320; Percent complete: 83.2%; Average loss: 3.1430\n",
            "Iteration: 8330; Percent complete: 83.3%; Average loss: 3.1682\n",
            "Iteration: 8340; Percent complete: 83.4%; Average loss: 3.2639\n",
            "Iteration: 8350; Percent complete: 83.5%; Average loss: 3.2056\n",
            "Iteration: 8360; Percent complete: 83.6%; Average loss: 3.1467\n",
            "Iteration: 8370; Percent complete: 83.7%; Average loss: 3.3578\n",
            "Iteration: 8380; Percent complete: 83.8%; Average loss: 3.2222\n",
            "Iteration: 8390; Percent complete: 83.9%; Average loss: 3.2699\n",
            "Iteration: 8400; Percent complete: 84.0%; Average loss: 3.3496\n",
            "Iteration: 8410; Percent complete: 84.1%; Average loss: 3.4753\n",
            "Iteration: 8420; Percent complete: 84.2%; Average loss: 3.1646\n",
            "Iteration: 8430; Percent complete: 84.3%; Average loss: 3.2721\n",
            "Iteration: 8440; Percent complete: 84.4%; Average loss: 3.3316\n",
            "Iteration: 8450; Percent complete: 84.5%; Average loss: 3.2304\n",
            "Iteration: 8460; Percent complete: 84.6%; Average loss: 3.2338\n",
            "Iteration: 8470; Percent complete: 84.7%; Average loss: 3.0997\n",
            "Iteration: 8480; Percent complete: 84.8%; Average loss: 3.1240\n",
            "Iteration: 8490; Percent complete: 84.9%; Average loss: 3.3914\n",
            "Iteration: 8500; Percent complete: 85.0%; Average loss: 3.2568\n",
            "Iteration: 8510; Percent complete: 85.1%; Average loss: 3.1362\n",
            "Iteration: 8520; Percent complete: 85.2%; Average loss: 3.3531\n",
            "Iteration: 8530; Percent complete: 85.3%; Average loss: 3.3303\n",
            "Iteration: 8540; Percent complete: 85.4%; Average loss: 3.0881\n",
            "Iteration: 8550; Percent complete: 85.5%; Average loss: 3.2782\n",
            "Iteration: 8560; Percent complete: 85.6%; Average loss: 3.3961\n",
            "Iteration: 8570; Percent complete: 85.7%; Average loss: 3.3711\n",
            "Iteration: 8580; Percent complete: 85.8%; Average loss: 3.3066\n",
            "Iteration: 8590; Percent complete: 85.9%; Average loss: 3.3243\n",
            "Iteration: 8600; Percent complete: 86.0%; Average loss: 3.2647\n",
            "Iteration: 8610; Percent complete: 86.1%; Average loss: 3.1854\n",
            "Iteration: 8620; Percent complete: 86.2%; Average loss: 3.2164\n",
            "Iteration: 8630; Percent complete: 86.3%; Average loss: 3.4072\n",
            "Iteration: 8640; Percent complete: 86.4%; Average loss: 3.3606\n",
            "Iteration: 8650; Percent complete: 86.5%; Average loss: 3.2599\n",
            "Iteration: 8660; Percent complete: 86.6%; Average loss: 3.2801\n",
            "Iteration: 8670; Percent complete: 86.7%; Average loss: 3.3391\n",
            "Iteration: 8680; Percent complete: 86.8%; Average loss: 3.3135\n",
            "Iteration: 8690; Percent complete: 86.9%; Average loss: 3.3031\n",
            "Iteration: 8700; Percent complete: 87.0%; Average loss: 3.2525\n",
            "Iteration: 8710; Percent complete: 87.1%; Average loss: 3.3313\n",
            "Iteration: 8720; Percent complete: 87.2%; Average loss: 3.2764\n",
            "Iteration: 8730; Percent complete: 87.3%; Average loss: 3.2432\n",
            "Iteration: 8740; Percent complete: 87.4%; Average loss: 3.3743\n",
            "Iteration: 8750; Percent complete: 87.5%; Average loss: 3.2525\n",
            "Iteration: 8760; Percent complete: 87.6%; Average loss: 3.4370\n",
            "Iteration: 8770; Percent complete: 87.7%; Average loss: 3.1887\n",
            "Iteration: 8780; Percent complete: 87.8%; Average loss: 3.4259\n",
            "Iteration: 8790; Percent complete: 87.9%; Average loss: 3.2460\n",
            "Iteration: 8800; Percent complete: 88.0%; Average loss: 3.2805\n",
            "Iteration: 8810; Percent complete: 88.1%; Average loss: 3.2659\n",
            "Iteration: 8820; Percent complete: 88.2%; Average loss: 3.2764\n",
            "Iteration: 8830; Percent complete: 88.3%; Average loss: 3.2535\n",
            "Iteration: 8840; Percent complete: 88.4%; Average loss: 3.1813\n",
            "Iteration: 8850; Percent complete: 88.5%; Average loss: 3.4582\n",
            "Iteration: 8860; Percent complete: 88.6%; Average loss: 3.1043\n",
            "Iteration: 8870; Percent complete: 88.7%; Average loss: 3.2839\n",
            "Iteration: 8880; Percent complete: 88.8%; Average loss: 3.3577\n",
            "Iteration: 8890; Percent complete: 88.9%; Average loss: 3.1428\n",
            "Iteration: 8900; Percent complete: 89.0%; Average loss: 3.2015\n",
            "Iteration: 8910; Percent complete: 89.1%; Average loss: 3.2072\n",
            "Iteration: 8920; Percent complete: 89.2%; Average loss: 3.3488\n",
            "Iteration: 8930; Percent complete: 89.3%; Average loss: 3.2755\n",
            "Iteration: 8940; Percent complete: 89.4%; Average loss: 3.3441\n",
            "Iteration: 8950; Percent complete: 89.5%; Average loss: 3.2547\n",
            "Iteration: 8960; Percent complete: 89.6%; Average loss: 3.1515\n",
            "Iteration: 8970; Percent complete: 89.7%; Average loss: 3.2266\n",
            "Iteration: 8980; Percent complete: 89.8%; Average loss: 3.3138\n",
            "Iteration: 8990; Percent complete: 89.9%; Average loss: 3.1715\n",
            "Iteration: 9000; Percent complete: 90.0%; Average loss: 3.3516\n",
            "Iteration: 9010; Percent complete: 90.1%; Average loss: 3.2445\n",
            "Iteration: 9020; Percent complete: 90.2%; Average loss: 3.2149\n",
            "Iteration: 9030; Percent complete: 90.3%; Average loss: 3.3008\n",
            "Iteration: 9040; Percent complete: 90.4%; Average loss: 3.1168\n",
            "Iteration: 9050; Percent complete: 90.5%; Average loss: 3.1261\n",
            "Iteration: 9060; Percent complete: 90.6%; Average loss: 3.2850\n",
            "Iteration: 9070; Percent complete: 90.7%; Average loss: 3.2877\n",
            "Iteration: 9080; Percent complete: 90.8%; Average loss: 3.2563\n",
            "Iteration: 9090; Percent complete: 90.9%; Average loss: 3.2443\n",
            "Iteration: 9100; Percent complete: 91.0%; Average loss: 3.1672\n",
            "Iteration: 9110; Percent complete: 91.1%; Average loss: 3.1241\n",
            "Iteration: 9120; Percent complete: 91.2%; Average loss: 3.4854\n",
            "Iteration: 9130; Percent complete: 91.3%; Average loss: 3.2405\n",
            "Iteration: 9140; Percent complete: 91.4%; Average loss: 3.3054\n",
            "Iteration: 9150; Percent complete: 91.5%; Average loss: 3.3665\n",
            "Iteration: 9160; Percent complete: 91.6%; Average loss: 3.4871\n",
            "Iteration: 9170; Percent complete: 91.7%; Average loss: 3.1380\n",
            "Iteration: 9180; Percent complete: 91.8%; Average loss: 3.2114\n",
            "Iteration: 9190; Percent complete: 91.9%; Average loss: 3.2839\n",
            "Iteration: 9200; Percent complete: 92.0%; Average loss: 3.2319\n",
            "Iteration: 9210; Percent complete: 92.1%; Average loss: 3.4436\n",
            "Iteration: 9220; Percent complete: 92.2%; Average loss: 3.3469\n",
            "Iteration: 9230; Percent complete: 92.3%; Average loss: 3.3061\n",
            "Iteration: 9240; Percent complete: 92.4%; Average loss: 3.0587\n",
            "Iteration: 9250; Percent complete: 92.5%; Average loss: 3.3080\n",
            "Iteration: 9260; Percent complete: 92.6%; Average loss: 3.3329\n",
            "Iteration: 9270; Percent complete: 92.7%; Average loss: 3.4416\n",
            "Iteration: 9280; Percent complete: 92.8%; Average loss: 3.3121\n",
            "Iteration: 9290; Percent complete: 92.9%; Average loss: 3.3595\n",
            "Iteration: 9300; Percent complete: 93.0%; Average loss: 3.2656\n",
            "Iteration: 9310; Percent complete: 93.1%; Average loss: 3.1565\n",
            "Iteration: 9320; Percent complete: 93.2%; Average loss: 3.4093\n",
            "Iteration: 9330; Percent complete: 93.3%; Average loss: 3.3417\n",
            "Iteration: 9340; Percent complete: 93.4%; Average loss: 3.1507\n",
            "Iteration: 9350; Percent complete: 93.5%; Average loss: 3.2329\n",
            "Iteration: 9360; Percent complete: 93.6%; Average loss: 3.2018\n",
            "Iteration: 9370; Percent complete: 93.7%; Average loss: 3.5268\n",
            "Iteration: 9380; Percent complete: 93.8%; Average loss: 3.4738\n",
            "Iteration: 9390; Percent complete: 93.9%; Average loss: 3.3749\n",
            "Iteration: 9400; Percent complete: 94.0%; Average loss: 3.1961\n",
            "Iteration: 9410; Percent complete: 94.1%; Average loss: 3.3921\n",
            "Iteration: 9420; Percent complete: 94.2%; Average loss: 3.4295\n",
            "Iteration: 9430; Percent complete: 94.3%; Average loss: 3.2879\n",
            "Iteration: 9440; Percent complete: 94.4%; Average loss: 3.1511\n",
            "Iteration: 9450; Percent complete: 94.5%; Average loss: 3.2601\n",
            "Iteration: 9460; Percent complete: 94.6%; Average loss: 3.2961\n",
            "Iteration: 9470; Percent complete: 94.7%; Average loss: 3.0917\n",
            "Iteration: 9480; Percent complete: 94.8%; Average loss: 3.4113\n",
            "Iteration: 9490; Percent complete: 94.9%; Average loss: 3.3446\n",
            "Iteration: 9500; Percent complete: 95.0%; Average loss: 3.1754\n",
            "Iteration: 9510; Percent complete: 95.1%; Average loss: 3.2148\n",
            "Iteration: 9520; Percent complete: 95.2%; Average loss: 3.1689\n",
            "Iteration: 9530; Percent complete: 95.3%; Average loss: 3.1669\n",
            "Iteration: 9540; Percent complete: 95.4%; Average loss: 3.1393\n",
            "Iteration: 9550; Percent complete: 95.5%; Average loss: 3.2727\n",
            "Iteration: 9560; Percent complete: 95.6%; Average loss: 3.2387\n",
            "Iteration: 9570; Percent complete: 95.7%; Average loss: 3.2448\n",
            "Iteration: 9580; Percent complete: 95.8%; Average loss: 3.1785\n",
            "Iteration: 9590; Percent complete: 95.9%; Average loss: 3.3028\n",
            "Iteration: 9600; Percent complete: 96.0%; Average loss: 3.2143\n",
            "Iteration: 9610; Percent complete: 96.1%; Average loss: 3.0382\n",
            "Iteration: 9620; Percent complete: 96.2%; Average loss: 3.3813\n",
            "Iteration: 9630; Percent complete: 96.3%; Average loss: 3.3230\n",
            "Iteration: 9640; Percent complete: 96.4%; Average loss: 3.3143\n",
            "Iteration: 9650; Percent complete: 96.5%; Average loss: 3.3012\n",
            "Iteration: 9660; Percent complete: 96.6%; Average loss: 3.2456\n",
            "Iteration: 9670; Percent complete: 96.7%; Average loss: 3.1663\n",
            "Iteration: 9680; Percent complete: 96.8%; Average loss: 3.2147\n",
            "Iteration: 9690; Percent complete: 96.9%; Average loss: 3.0932\n",
            "Iteration: 9700; Percent complete: 97.0%; Average loss: 3.3281\n",
            "Iteration: 9710; Percent complete: 97.1%; Average loss: 3.4035\n",
            "Iteration: 9720; Percent complete: 97.2%; Average loss: 3.4532\n",
            "Iteration: 9730; Percent complete: 97.3%; Average loss: 3.3439\n",
            "Iteration: 9740; Percent complete: 97.4%; Average loss: 3.2164\n",
            "Iteration: 9750; Percent complete: 97.5%; Average loss: 3.3903\n",
            "Iteration: 9760; Percent complete: 97.6%; Average loss: 3.1341\n",
            "Iteration: 9770; Percent complete: 97.7%; Average loss: 3.2229\n",
            "Iteration: 9780; Percent complete: 97.8%; Average loss: 3.2854\n",
            "Iteration: 9790; Percent complete: 97.9%; Average loss: 3.1872\n",
            "Iteration: 9800; Percent complete: 98.0%; Average loss: 3.1601\n",
            "Iteration: 9810; Percent complete: 98.1%; Average loss: 3.1180\n",
            "Iteration: 9820; Percent complete: 98.2%; Average loss: 3.4454\n",
            "Iteration: 9830; Percent complete: 98.3%; Average loss: 3.1530\n",
            "Iteration: 9840; Percent complete: 98.4%; Average loss: 3.2258\n",
            "Iteration: 9850; Percent complete: 98.5%; Average loss: 3.2461\n",
            "Iteration: 9860; Percent complete: 98.6%; Average loss: 3.2316\n",
            "Iteration: 9870; Percent complete: 98.7%; Average loss: 3.3995\n",
            "Iteration: 9880; Percent complete: 98.8%; Average loss: 3.3072\n",
            "Iteration: 9890; Percent complete: 98.9%; Average loss: 3.4505\n",
            "Iteration: 9900; Percent complete: 99.0%; Average loss: 3.1480\n",
            "Iteration: 9910; Percent complete: 99.1%; Average loss: 3.2541\n",
            "Iteration: 9920; Percent complete: 99.2%; Average loss: 3.2042\n",
            "Iteration: 9930; Percent complete: 99.3%; Average loss: 3.4094\n",
            "Iteration: 9940; Percent complete: 99.4%; Average loss: 3.1484\n",
            "Iteration: 9950; Percent complete: 99.5%; Average loss: 3.4410\n",
            "Iteration: 9960; Percent complete: 99.6%; Average loss: 3.1696\n",
            "Iteration: 9970; Percent complete: 99.7%; Average loss: 3.3325\n",
            "Iteration: 9980; Percent complete: 99.8%; Average loss: 3.2534\n",
            "Iteration: 9990; Percent complete: 99.9%; Average loss: 3.2947\n",
            "Iteration: 10000; Percent complete: 100.0%; Average loss: 3.2007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIzRFQK0KeNR"
      },
      "source": [
        "## Evaluation ( Function to start chatbot agent )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igNKWLwCQyiu"
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfFQz3b5dGRL"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting\n",
        "#evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6iDJLFNGWtI"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynR1-itTKW23"
      },
      "source": [
        "## Defining models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZ_ZINmKJIY"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMOvsT5tXBsy"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmSHZpBsKLMi"
      },
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60NZu-MNXCKC"
      },
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv0QYJpHKOCv"
      },
      "source": [
        "### Decoder with Policy Gradient Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH5J5KhzXi3x"
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLdfzQ8kYGmZ",
        "outputId": "5c854a5b-8045-4ea3-e152-200fddbf5531"
      },
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 10000 # 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "#print(loadFilename)\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    #checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRvg4cLyXtkR"
      },
      "source": [
        "forward_encoder = encoder\n",
        "forward_decoder = decoder\n",
        "forward_encoder = forward_encoder.to(device)\n",
        "forward_decoder = forward_decoder.to(device)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QutoJjj9Xzr4"
      },
      "source": [
        "backward_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "backward_decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "backward_encoder = backward_encoder.to(device)\n",
        "backward_decoder = backward_decoder.to(device)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdVV4-apYIIN"
      },
      "source": [
        "def convertResponse(response):\n",
        "    size1 = len(response)\n",
        "    size2 = batch_size\n",
        "    npRes = np.zeros((size1, size2), dtype=np.int64)\n",
        "    npLengths = np.zeros(size2, dtype=np.int64)\n",
        "    for i in range(size1):\n",
        "        prov = response[i].cpu().numpy()\n",
        "        for j in range(prov.size):\n",
        "            npLengths[j] = npLengths[j] + 1\n",
        "            if prov.size > 1:\n",
        "                npRes[i][j] = prov[j]\n",
        "            else:\n",
        "                npRes[i][j] = prov \n",
        "    res = torch.from_numpy(npRes)\n",
        "    lengths = torch.from_numpy(npLengths)\n",
        "    return res, lengths"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPKfqSEwYLNt"
      },
      "source": [
        "def convertTarget(target):\n",
        "    size1 = len(target)\n",
        "    size2 = batch_size\n",
        "    npRes = np.zeros((size1, size2), dtype=np.int64)\n",
        "    mask = np.zeros((size1, size2), dtype=np.bool_)\n",
        "    npLengths = np.zeros(size2, dtype=np.int64)\n",
        "    for i in range(size1):\n",
        "        prov = target[i].cpu().numpy()\n",
        "        for j in range(prov.size):\n",
        "            npLengths[j] = npLengths[j] + 1\n",
        "            if prov.size > 1:\n",
        "                npRes[i][j] = prov[j]\n",
        "            else:\n",
        "                npRes[i][j] = prov \n",
        "                \n",
        "            if npRes[i][j] > 0:\n",
        "                mask[i][j] = True\n",
        "            else:\n",
        "                mask[i][j] = False\n",
        "            \n",
        "    res = torch.from_numpy(npRes)\n",
        "    lengths = torch.from_numpy(npLengths)\n",
        "    mask= torch.from_numpy(mask)\n",
        "    max_target_len = torch.max(lengths) #.detach().numpy()\n",
        "    return res, mask, max_target_len"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLuBEq_EYNS4"
      },
      "source": [
        "def transformTensorToSameShapeAs(tensor, shape):\n",
        "    size1, size2 = shape\n",
        "    npNewT = np.zeros((size1, size2), dtype=np.int64)\n",
        "    npNewMask = np.zeros((size1, size2), dtype=np.bool_)\n",
        "    tensorSize1, tensorSize2 = tensor.size()\n",
        "    for i in range(tensorSize1):\n",
        "        for j in range(tensorSize2):\n",
        "            npNewT[i][j] = tensor[i][j]\n",
        "            npNewMask[i][j]= True\n",
        "    return torch.from_numpy(npNewT), torch.from_numpy(npNewMask)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3IzGWOKpIm"
      },
      "source": [
        "## Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG2UF13SYO8Y"
      },
      "source": [
        "def RL_agent(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio):\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    \n",
        "    #Initialize variables\n",
        "    loss=0\n",
        "    #print_losses = []\n",
        "    response=[]\n",
        "    \n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "    \n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    \n",
        "    \n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "     # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "     # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            #print_losses.append(mask_loss.item() * nTotal)\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            #print_losses.append(mask_loss.item() * nTotal)\n",
        "            \n",
        "            #ni or decoder_output\n",
        "            response.append(topi)\n",
        "            \n",
        "    return loss, max_target_len, response"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON7XGiQ0S5G5"
      },
      "source": [
        "### Rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wszkDrW7YRnd"
      },
      "source": [
        "def easeOfAnswering(input_variable, lengths, dull_responses, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio):\n",
        "    NS=len(dull_responses)\n",
        "    r1=0\n",
        "    for d in dull_responses:\n",
        "      forward_loss, forward_len, _ = RL(input_variable, lengths, newD, newMask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio)\n",
        "      if forward_len > 0:\n",
        "          r1 -= forward_loss / forward_len\n",
        "    if len(dull_responses) > 0:\n",
        "        r1 = r1 / NS\n",
        "    return r1\n",
        "\n",
        "def informationFlow(responses):\n",
        "    r2=0\n",
        "  \n",
        "    vec_a = responses[-3].data\n",
        "    vec_b = responses[-1].data\n",
        "\n",
        "    # length of the two vector might not match\n",
        "    min_length = min(len(vec_a), len(vec_b))\n",
        "    vec_a = vec_a[:min_length]\n",
        "    vec_b = vec_b[:min_length]\n",
        "    cos_sim = 1 - scipy.spatial.distance.cosine(vec_a, vec_b)\n",
        "\n",
        "    if cos_sim <= 0:\n",
        "        r2 = - cos_sim\n",
        "    else:\n",
        "        r2 = - np.log(cos_sim)\n",
        "    return r2\n",
        "\n",
        "def semanticCoherence(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio):\n",
        "   \n",
        "    r3 = 0\n",
        "    forward_loss, forward_len, _ = RL(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "    ep_input, lengths_trans = convertResponse(target_variable)\n",
        "\n",
        "    ep_target, mask_trans, max_target_len_trans = convertTarget(input_variable)\n",
        "    \n",
        "    backward_loss, backward_len, _ = RL(ep_input, lengths_trans, ep_target, mask_trans, max_target_len_trans, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
        "    if forward_len > 0:\n",
        "        r3 += forward_loss / forward_len\n",
        "    if backward_len > 0:\n",
        "        r3+= backward_loss / backward_len\n",
        "    return r3"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTx-BzjIS3Bt"
      },
      "source": [
        "### Episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-6CLczqYUWl"
      },
      "source": [
        "l1=0.25\n",
        "l2=0.25\n",
        "l3=0.5"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yneS-wOqYXEl"
      },
      "source": [
        "def episode_step(input_var, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio):\n",
        "    #rewards per episode\n",
        "    ep_rewards = []\n",
        "    #indice of current episode\n",
        "    ep_num = 1\n",
        "    #list of responses\n",
        "    responses = []\n",
        "    #input of current episode\n",
        "    ep_input = input_var\n",
        "    #target of current episode\n",
        "    ep_target = target_var\n",
        "    \n",
        "    #ep_num bounded -> to redefine (MEDIUM POST)\n",
        "    while (ep_num <= 10):\n",
        "        \n",
        "        print(ep_num)\n",
        "        #generate current response with the forward model\n",
        "        _, _, curr_response = RL(ep_input, lengths, ep_target, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        if(len(curr_response) < MIN_COUNT):# or (curr_response in dull_responses) or (curr_response in responses)):\n",
        "            break\n",
        "        \n",
        "        #Ease of answering\n",
        "        r1 = easeOfAnswering(ep_input, lengths, dull_responses, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Information flow\n",
        "        r2 = informationFlow(responses)\n",
        "        \n",
        "        #Semantic coherence\n",
        "        r3 = semanticCoherence(ep_input, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Final reward as a weighted sum of rewards\n",
        "        r = l1*r1 + l2*r2 + l3*r3\n",
        "        \n",
        "        #Add the current reward to the list\n",
        "        ep_rewards.append(r.detach().cpu().numpy())\n",
        "        \n",
        "        #We can add the response to responses list\n",
        "        curr_response, lengths = convertResponse(curr_response)\n",
        "        curr_response = curr_response.to(device)\n",
        "        responses.append(curr_response)\n",
        "        \n",
        "        #Next input is the current response\n",
        "        ep_input = curr_response\n",
        "        #Next target -> dummy\n",
        "        ep_target = torch.zeros(MAX_LENGTH,batch_size,dtype=torch.int64)\n",
        "        #ep_target = torch.LongTensor(torch.LongTensor([0] * MAX_LENGTH)).view(-1, 1)\n",
        "        ep_target = ep_target.to(device)\n",
        "        \n",
        "        #Turn off the teacher forcing  after first iteration -> dummy target\n",
        "        teacher_forcing_ratio = 0\n",
        "        ep_num +=1\n",
        "        \n",
        "    #Take the mean of the episodic rewards\n",
        "    return np.mean(ep_rewards) if len(ep_rewards) > 0 else 0 "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vcyjaXjYZ3k"
      },
      "source": [
        "dull_responses = [\"i do not know what you are talking about.\", \"i do not know.\", \n",
        " \"you do not know.\", \"you know what i mean.\", \"i know what you mean.\", \n",
        " \"you know what i am saying.\", \"you do not know anything.\"]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctuLqSQSoyc"
      },
      "source": [
        "### Training and updating Agent with Rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_NV4x5DYbol"
      },
      "source": [
        "def trainingRL(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer,teacher_forcing_ratio, dull_responses, n_iteration, print_every, save_every, save_dir):\n",
        "    \n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "    \n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    \n",
        "    \n",
        "    #Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        print(\"Iteration\", iteration)\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        ##MODIFS HERE\n",
        "        # Zero gradients the optimizer\n",
        "        forward_encoder_optimizer.zero_grad()\n",
        "        forward_decoder_optimizer.zero_grad()\n",
        "        \n",
        "        backward_encoder_optimizer.zero_grad()\n",
        "        backward_decoder_optimizer.zero_grad()\n",
        "        \n",
        "        #Forward\n",
        "        forward_loss, forward_len, _ = RL_agent(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Calculate reward\n",
        "        reward = episode_step(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Update forward seq2seq with loss scaled by reward\n",
        "        loss = forward_loss * reward\n",
        "        \n",
        "        loss.backward()\n",
        "        forward_encoder_optimizer.step()\n",
        "        forward_decoder_optimizer.step()\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        print_loss += loss / forward_len\n",
        "        \n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B294mBNJagtB",
        "outputId": "cf3b602a-e7bb-4db9-8298-947c31ef5ce6"
      },
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 10000 #4000\n",
        "print_every = 1\n",
        "save_every = 1000\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimizers ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3rPvvXQSjDv"
      },
      "source": [
        "### Running an Episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgUEJsEWYd4k",
        "outputId": "2b82574d-27ee-457d-fc1c-5719d434e8c1"
      },
      "source": [
        "#Configure RL model\n",
        "\n",
        "model_name='RL_model_seq'\n",
        "n_iteration = 10000\n",
        "print_every=100\n",
        "save_every=500\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "forward_encoder.train()\n",
        "forward_decoder.train()\n",
        "\n",
        "backward_encoder.train()\n",
        "backward_decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "forward_encoder_optimizer = optim.Adam(forward_encoder.parameters(), lr=learning_rate)\n",
        "forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "backward_encoder_optimizer = optim.Adam(backward_encoder.parameters(), lr=learning_rate)\n",
        "backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "            \n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainingRL(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer, teacher_forcing_ratio, dull_responses, n_iteration, print_every, save_every, save_dir)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9217\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9218\n",
            "1\n",
            "Iteration 9219\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9220\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9221\n",
            "1\n",
            "Iteration 9222\n",
            "1\n",
            "Iteration 9223\n",
            "1\n",
            "Iteration 9224\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9225\n",
            "1\n",
            "Iteration 9226\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9227\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9228\n",
            "1\n",
            "Iteration 9229\n",
            "1\n",
            "Iteration 9230\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9231\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9232\n",
            "1\n",
            "Iteration 9233\n",
            "1\n",
            "Iteration 9234\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9235\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9236\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9237\n",
            "1\n",
            "Iteration 9238\n",
            "1\n",
            "Iteration 9239\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9240\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9241\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9242\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9243\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9244\n",
            "1\n",
            "Iteration 9245\n",
            "1\n",
            "Iteration 9246\n",
            "1\n",
            "Iteration 9247\n",
            "1\n",
            "Iteration 9248\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9249\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9250\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9251\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9252\n",
            "1\n",
            "Iteration 9253\n",
            "1\n",
            "Iteration 9254\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9255\n",
            "1\n",
            "Iteration 9256\n",
            "1\n",
            "Iteration 9257\n",
            "1\n",
            "Iteration 9258\n",
            "1\n",
            "Iteration 9259\n",
            "1\n",
            "Iteration 9260\n",
            "1\n",
            "Iteration 9261\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9262\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9263\n",
            "1\n",
            "Iteration 9264\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9265\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9266\n",
            "1\n",
            "Iteration 9267\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9268\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9269\n",
            "1\n",
            "Iteration 9270\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9271\n",
            "1\n",
            "Iteration 9272\n",
            "1\n",
            "Iteration 9273\n",
            "1\n",
            "Iteration 9274\n",
            "1\n",
            "Iteration 9275\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9276\n",
            "1\n",
            "Iteration 9277\n",
            "1\n",
            "Iteration 9278\n",
            "1\n",
            "Iteration 9279\n",
            "1\n",
            "Iteration 9280\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9281\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9282\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9283\n",
            "1\n",
            "Iteration 9284\n",
            "1\n",
            "Iteration 9285\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9286\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9287\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9288\n",
            "1\n",
            "Iteration 9289\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9290\n",
            "1\n",
            "Iteration 9291\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9292\n",
            "1\n",
            "Iteration 9293\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9294\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9295\n",
            "1\n",
            "Iteration 9296\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9297\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9298\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9299\n",
            "1\n",
            "Iteration 9300\n",
            "1\n",
            "Iteration: 9300; Percent complete: 93.0%; Average loss: 4.8051\n",
            "Iteration 9301\n",
            "1\n",
            "Iteration 9302\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9303\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9304\n",
            "1\n",
            "Iteration 9305\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9306\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9307\n",
            "1\n",
            "Iteration 9308\n",
            "1\n",
            "Iteration 9309\n",
            "1\n",
            "Iteration 9310\n",
            "1\n",
            "Iteration 9311\n",
            "1\n",
            "Iteration 9312\n",
            "1\n",
            "Iteration 9313\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9314\n",
            "1\n",
            "Iteration 9315\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9316\n",
            "1\n",
            "Iteration 9317\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9318\n",
            "1\n",
            "Iteration 9319\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9320\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9321\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9322\n",
            "1\n",
            "Iteration 9323\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9324\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9325\n",
            "1\n",
            "Iteration 9326\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9327\n",
            "1\n",
            "Iteration 9328\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9329\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9330\n",
            "1\n",
            "Iteration 9331\n",
            "1\n",
            "Iteration 9332\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9333\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9334\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9335\n",
            "1\n",
            "Iteration 9336\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9337\n",
            "1\n",
            "Iteration 9338\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9339\n",
            "1\n",
            "Iteration 9340\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9341\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9342\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9343\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9344\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9345\n",
            "1\n",
            "Iteration 9346\n",
            "1\n",
            "Iteration 9347\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9348\n",
            "1\n",
            "Iteration 9349\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9350\n",
            "1\n",
            "Iteration 9351\n",
            "1\n",
            "Iteration 9352\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9353\n",
            "1\n",
            "Iteration 9354\n",
            "1\n",
            "Iteration 9355\n",
            "1\n",
            "Iteration 9356\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9357\n",
            "1\n",
            "Iteration 9358\n",
            "1\n",
            "Iteration 9359\n",
            "1\n",
            "Iteration 9360\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9361\n",
            "1\n",
            "Iteration 9362\n",
            "1\n",
            "Iteration 9363\n",
            "1\n",
            "Iteration 9364\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9365\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9366\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9367\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9368\n",
            "1\n",
            "Iteration 9369\n",
            "1\n",
            "Iteration 9370\n",
            "1\n",
            "Iteration 9371\n",
            "1\n",
            "Iteration 9372\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9373\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9374\n",
            "1\n",
            "Iteration 9375\n",
            "1\n",
            "Iteration 9376\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9377\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9378\n",
            "1\n",
            "Iteration 9379\n",
            "1\n",
            "Iteration 9380\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9381\n",
            "1\n",
            "Iteration 9382\n",
            "1\n",
            "Iteration 9383\n",
            "1\n",
            "Iteration 9384\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9385\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9386\n",
            "1\n",
            "Iteration 9387\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9388\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9389\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9390\n",
            "1\n",
            "Iteration 9391\n",
            "1\n",
            "Iteration 9392\n",
            "1\n",
            "Iteration 9393\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9394\n",
            "1\n",
            "Iteration 9395\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9396\n",
            "1\n",
            "Iteration 9397\n",
            "1\n",
            "Iteration 9398\n",
            "1\n",
            "Iteration 9399\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9400\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration: 9400; Percent complete: 94.0%; Average loss: 5.0167\n",
            "Iteration 9401\n",
            "1\n",
            "Iteration 9402\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9403\n",
            "1\n",
            "Iteration 9404\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9405\n",
            "1\n",
            "Iteration 9406\n",
            "1\n",
            "Iteration 9407\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9408\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9409\n",
            "1\n",
            "Iteration 9410\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9411\n",
            "1\n",
            "Iteration 9412\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9413\n",
            "1\n",
            "Iteration 9414\n",
            "1\n",
            "Iteration 9415\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9416\n",
            "1\n",
            "Iteration 9417\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9418\n",
            "1\n",
            "Iteration 9419\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9420\n",
            "1\n",
            "Iteration 9421\n",
            "1\n",
            "Iteration 9422\n",
            "1\n",
            "Iteration 9423\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9424\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9425\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9426\n",
            "1\n",
            "Iteration 9427\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9428\n",
            "1\n",
            "Iteration 9429\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9430\n",
            "1\n",
            "Iteration 9431\n",
            "1\n",
            "Iteration 9432\n",
            "1\n",
            "Iteration 9433\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9434\n",
            "1\n",
            "Iteration 9435\n",
            "1\n",
            "Iteration 9436\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9437\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9438\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9439\n",
            "1\n",
            "Iteration 9440\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9441\n",
            "1\n",
            "Iteration 9442\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9443\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9444\n",
            "1\n",
            "Iteration 9445\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9446\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9447\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9448\n",
            "1\n",
            "Iteration 9449\n",
            "1\n",
            "Iteration 9450\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9451\n",
            "1\n",
            "Iteration 9452\n",
            "1\n",
            "Iteration 9453\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9454\n",
            "1\n",
            "Iteration 9455\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9456\n",
            "1\n",
            "Iteration 9457\n",
            "1\n",
            "Iteration 9458\n",
            "1\n",
            "Iteration 9459\n",
            "1\n",
            "Iteration 9460\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9461\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9462\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9463\n",
            "1\n",
            "Iteration 9464\n",
            "1\n",
            "Iteration 9465\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9466\n",
            "1\n",
            "Iteration 9467\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9468\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9469\n",
            "1\n",
            "Iteration 9470\n",
            "1\n",
            "Iteration 9471\n",
            "1\n",
            "Iteration 9472\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9473\n",
            "1\n",
            "Iteration 9474\n",
            "1\n",
            "Iteration 9475\n",
            "1\n",
            "Iteration 9476\n",
            "1\n",
            "Iteration 9477\n",
            "1\n",
            "Iteration 9478\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9479\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9480\n",
            "1\n",
            "Iteration 9481\n",
            "1\n",
            "Iteration 9482\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9483\n",
            "1\n",
            "Iteration 9484\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9485\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9486\n",
            "1\n",
            "Iteration 9487\n",
            "1\n",
            "Iteration 9488\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9489\n",
            "1\n",
            "Iteration 9490\n",
            "1\n",
            "Iteration 9491\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9492\n",
            "1\n",
            "Iteration 9493\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9494\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9495\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9496\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9497\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9498\n",
            "1\n",
            "Iteration 9499\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9500\n",
            "1\n",
            "Iteration: 9500; Percent complete: 95.0%; Average loss: 4.7597\n",
            "Iteration 9501\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9502\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9503\n",
            "1\n",
            "Iteration 9504\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9505\n",
            "1\n",
            "Iteration 9506\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9507\n",
            "1\n",
            "Iteration 9508\n",
            "1\n",
            "Iteration 9509\n",
            "1\n",
            "Iteration 9510\n",
            "1\n",
            "Iteration 9511\n",
            "1\n",
            "Iteration 9512\n",
            "1\n",
            "Iteration 9513\n",
            "1\n",
            "Iteration 9514\n",
            "1\n",
            "Iteration 9515\n",
            "1\n",
            "Iteration 9516\n",
            "1\n",
            "Iteration 9517\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9518\n",
            "1\n",
            "Iteration 9519\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9520\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9521\n",
            "1\n",
            "Iteration 9522\n",
            "1\n",
            "Iteration 9523\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9524\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9525\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9526\n",
            "1\n",
            "Iteration 9527\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9528\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9529\n",
            "1\n",
            "Iteration 9530\n",
            "1\n",
            "Iteration 9531\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9532\n",
            "1\n",
            "Iteration 9533\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9534\n",
            "1\n",
            "Iteration 9535\n",
            "1\n",
            "Iteration 9536\n",
            "1\n",
            "Iteration 9537\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9538\n",
            "1\n",
            "Iteration 9539\n",
            "1\n",
            "Iteration 9540\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9541\n",
            "1\n",
            "Iteration 9542\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9543\n",
            "1\n",
            "Iteration 9544\n",
            "1\n",
            "Iteration 9545\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9546\n",
            "1\n",
            "Iteration 9547\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9548\n",
            "1\n",
            "Iteration 9549\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9550\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9551\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9552\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9553\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9554\n",
            "1\n",
            "Iteration 9555\n",
            "1\n",
            "Iteration 9556\n",
            "1\n",
            "Iteration 9557\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9558\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9559\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9560\n",
            "1\n",
            "Iteration 9561\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9562\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9563\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9564\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9565\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9566\n",
            "1\n",
            "Iteration 9567\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9568\n",
            "1\n",
            "Iteration 9569\n",
            "1\n",
            "Iteration 9570\n",
            "1\n",
            "Iteration 9571\n",
            "1\n",
            "Iteration 9572\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9573\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9574\n",
            "1\n",
            "Iteration 9575\n",
            "1\n",
            "Iteration 9576\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9577\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9578\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9579\n",
            "1\n",
            "Iteration 9580\n",
            "1\n",
            "Iteration 9581\n",
            "1\n",
            "Iteration 9582\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9583\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9584\n",
            "1\n",
            "Iteration 9585\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9586\n",
            "1\n",
            "Iteration 9587\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9588\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9589\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9590\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9591\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9592\n",
            "1\n",
            "Iteration 9593\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9594\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9595\n",
            "1\n",
            "Iteration 9596\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9597\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9598\n",
            "1\n",
            "Iteration 9599\n",
            "1\n",
            "Iteration 9600\n",
            "1\n",
            "Iteration: 9600; Percent complete: 96.0%; Average loss: 4.9481\n",
            "Iteration 9601\n",
            "1\n",
            "Iteration 9602\n",
            "1\n",
            "Iteration 9603\n",
            "1\n",
            "Iteration 9604\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9605\n",
            "1\n",
            "Iteration 9606\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9607\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9608\n",
            "1\n",
            "Iteration 9609\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9610\n",
            "1\n",
            "Iteration 9611\n",
            "1\n",
            "Iteration 9612\n",
            "1\n",
            "Iteration 9613\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9614\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9615\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9616\n",
            "1\n",
            "Iteration 9617\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9618\n",
            "1\n",
            "Iteration 9619\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9620\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9621\n",
            "1\n",
            "Iteration 9622\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9623\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9624\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9625\n",
            "1\n",
            "Iteration 9626\n",
            "1\n",
            "Iteration 9627\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9628\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9629\n",
            "1\n",
            "Iteration 9630\n",
            "1\n",
            "Iteration 9631\n",
            "1\n",
            "Iteration 9632\n",
            "1\n",
            "Iteration 9633\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9634\n",
            "1\n",
            "Iteration 9635\n",
            "1\n",
            "Iteration 9636\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9637\n",
            "1\n",
            "Iteration 9638\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9639\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9640\n",
            "1\n",
            "Iteration 9641\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9642\n",
            "1\n",
            "Iteration 9643\n",
            "1\n",
            "Iteration 9644\n",
            "1\n",
            "Iteration 9645\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9646\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9647\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9648\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9649\n",
            "1\n",
            "Iteration 9650\n",
            "1\n",
            "Iteration 9651\n",
            "1\n",
            "Iteration 9652\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9653\n",
            "1\n",
            "Iteration 9654\n",
            "1\n",
            "Iteration 9655\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9656\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9657\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9658\n",
            "1\n",
            "Iteration 9659\n",
            "1\n",
            "Iteration 9660\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9661\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9662\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9663\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9664\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9665\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9666\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9667\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9668\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9669\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9670\n",
            "1\n",
            "Iteration 9671\n",
            "1\n",
            "Iteration 9672\n",
            "1\n",
            "Iteration 9673\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9674\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9675\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9676\n",
            "1\n",
            "Iteration 9677\n",
            "1\n",
            "Iteration 9678\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9679\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9680\n",
            "1\n",
            "Iteration 9681\n",
            "1\n",
            "Iteration 9682\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9683\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9684\n",
            "1\n",
            "Iteration 9685\n",
            "1\n",
            "Iteration 9686\n",
            "1\n",
            "Iteration 9687\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9688\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9689\n",
            "1\n",
            "Iteration 9690\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9691\n",
            "1\n",
            "Iteration 9692\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9693\n",
            "1\n",
            "Iteration 9694\n",
            "1\n",
            "Iteration 9695\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9696\n",
            "1\n",
            "Iteration 9697\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9698\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9699\n",
            "1\n",
            "Iteration 9700\n",
            "1\n",
            "Iteration: 9700; Percent complete: 97.0%; Average loss: 5.1134\n",
            "Iteration 9701\n",
            "1\n",
            "Iteration 9702\n",
            "1\n",
            "Iteration 9703\n",
            "1\n",
            "Iteration 9704\n",
            "1\n",
            "Iteration 9705\n",
            "1\n",
            "Iteration 9706\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9707\n",
            "1\n",
            "Iteration 9708\n",
            "1\n",
            "Iteration 9709\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9710\n",
            "1\n",
            "Iteration 9711\n",
            "1\n",
            "Iteration 9712\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9713\n",
            "1\n",
            "Iteration 9714\n",
            "1\n",
            "Iteration 9715\n",
            "1\n",
            "Iteration 9716\n",
            "1\n",
            "Iteration 9717\n",
            "1\n",
            "Iteration 9718\n",
            "1\n",
            "Iteration 9719\n",
            "1\n",
            "Iteration 9720\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9721\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9722\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9723\n",
            "1\n",
            "Iteration 9724\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9725\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9726\n",
            "1\n",
            "Iteration 9727\n",
            "1\n",
            "Iteration 9728\n",
            "1\n",
            "Iteration 9729\n",
            "1\n",
            "Iteration 9730\n",
            "1\n",
            "Iteration 9731\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9732\n",
            "1\n",
            "Iteration 9733\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9734\n",
            "1\n",
            "Iteration 9735\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9736\n",
            "1\n",
            "Iteration 9737\n",
            "1\n",
            "Iteration 9738\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9739\n",
            "1\n",
            "Iteration 9740\n",
            "1\n",
            "Iteration 9741\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9742\n",
            "1\n",
            "Iteration 9743\n",
            "1\n",
            "Iteration 9744\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9745\n",
            "1\n",
            "Iteration 9746\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9747\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9748\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9749\n",
            "1\n",
            "Iteration 9750\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9751\n",
            "1\n",
            "Iteration 9752\n",
            "1\n",
            "Iteration 9753\n",
            "1\n",
            "Iteration 9754\n",
            "1\n",
            "Iteration 9755\n",
            "1\n",
            "Iteration 9756\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9757\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9758\n",
            "1\n",
            "Iteration 9759\n",
            "1\n",
            "Iteration 9760\n",
            "1\n",
            "Iteration 9761\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9762\n",
            "1\n",
            "Iteration 9763\n",
            "1\n",
            "Iteration 9764\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9765\n",
            "1\n",
            "Iteration 9766\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9767\n",
            "1\n",
            "Iteration 9768\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9769\n",
            "1\n",
            "Iteration 9770\n",
            "1\n",
            "Iteration 9771\n",
            "1\n",
            "Iteration 9772\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9773\n",
            "1\n",
            "Iteration 9774\n",
            "1\n",
            "Iteration 9775\n",
            "1\n",
            "Iteration 9776\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9777\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9778\n",
            "1\n",
            "Iteration 9779\n",
            "1\n",
            "Iteration 9780\n",
            "1\n",
            "Iteration 9781\n",
            "1\n",
            "Iteration 9782\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9783\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9784\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9785\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9786\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9787\n",
            "1\n",
            "Iteration 9788\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9789\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9790\n",
            "1\n",
            "Iteration 9791\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9792\n",
            "1\n",
            "Iteration 9793\n",
            "1\n",
            "Iteration 9794\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9795\n",
            "1\n",
            "Iteration 9796\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9797\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9798\n",
            "1\n",
            "Iteration 9799\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9800\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration: 9800; Percent complete: 98.0%; Average loss: 3.7940\n",
            "Iteration 9801\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9802\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9803\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9804\n",
            "1\n",
            "Iteration 9805\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9806\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9807\n",
            "1\n",
            "Iteration 9808\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9809\n",
            "1\n",
            "Iteration 9810\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9811\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9812\n",
            "1\n",
            "Iteration 9813\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9814\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9815\n",
            "1\n",
            "Iteration 9816\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9817\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9818\n",
            "1\n",
            "Iteration 9819\n",
            "1\n",
            "Iteration 9820\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9821\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9822\n",
            "1\n",
            "Iteration 9823\n",
            "1\n",
            "Iteration 9824\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9825\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9826\n",
            "1\n",
            "Iteration 9827\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9828\n",
            "1\n",
            "Iteration 9829\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9830\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9831\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9832\n",
            "1\n",
            "Iteration 9833\n",
            "1\n",
            "Iteration 9834\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9835\n",
            "1\n",
            "Iteration 9836\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9837\n",
            "1\n",
            "Iteration 9838\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9839\n",
            "1\n",
            "Iteration 9840\n",
            "1\n",
            "Iteration 9841\n",
            "1\n",
            "Iteration 9842\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9843\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9844\n",
            "1\n",
            "Iteration 9845\n",
            "1\n",
            "Iteration 9846\n",
            "1\n",
            "Iteration 9847\n",
            "1\n",
            "Iteration 9848\n",
            "1\n",
            "Iteration 9849\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9850\n",
            "1\n",
            "Iteration 9851\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9852\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9853\n",
            "1\n",
            "Iteration 9854\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9855\n",
            "1\n",
            "Iteration 9856\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9857\n",
            "1\n",
            "Iteration 9858\n",
            "1\n",
            "Iteration 9859\n",
            "1\n",
            "Iteration 9860\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9861\n",
            "1\n",
            "Iteration 9862\n",
            "1\n",
            "Iteration 9863\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9864\n",
            "1\n",
            "Iteration 9865\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9866\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9867\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9868\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9869\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9870\n",
            "1\n",
            "Iteration 9871\n",
            "1\n",
            "Iteration 9872\n",
            "1\n",
            "Iteration 9873\n",
            "1\n",
            "Iteration 9874\n",
            "1\n",
            "Iteration 9875\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9876\n",
            "1\n",
            "Iteration 9877\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9878\n",
            "1\n",
            "Iteration 9879\n",
            "1\n",
            "Iteration 9880\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9881\n",
            "1\n",
            "Iteration 9882\n",
            "1\n",
            "Iteration 9883\n",
            "1\n",
            "Iteration 9884\n",
            "1\n",
            "Iteration 9885\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9886\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9887\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9888\n",
            "1\n",
            "Iteration 9889\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9890\n",
            "1\n",
            "Iteration 9891\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9892\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9893\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9894\n",
            "1\n",
            "Iteration 9895\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9896\n",
            "1\n",
            "Iteration 9897\n",
            "1\n",
            "Iteration 9898\n",
            "1\n",
            "Iteration 9899\n",
            "1\n",
            "Iteration 9900\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration: 9900; Percent complete: 99.0%; Average loss: 4.8846\n",
            "Iteration 9901\n",
            "1\n",
            "Iteration 9902\n",
            "1\n",
            "Iteration 9903\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9904\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9905\n",
            "1\n",
            "Iteration 9906\n",
            "1\n",
            "Iteration 9907\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9908\n",
            "1\n",
            "Iteration 9909\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9910\n",
            "1\n",
            "Iteration 9911\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9912\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9913\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9914\n",
            "1\n",
            "Iteration 9915\n",
            "1\n",
            "Iteration 9916\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9917\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9918\n",
            "1\n",
            "Iteration 9919\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9920\n",
            "1\n",
            "Iteration 9921\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9922\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9923\n",
            "1\n",
            "Iteration 9924\n",
            "1\n",
            "Iteration 9925\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9926\n",
            "1\n",
            "Iteration 9927\n",
            "1\n",
            "Iteration 9928\n",
            "1\n",
            "Iteration 9929\n",
            "1\n",
            "Iteration 9930\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9931\n",
            "1\n",
            "Iteration 9932\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9933\n",
            "1\n",
            "Iteration 9934\n",
            "1\n",
            "Iteration 9935\n",
            "1\n",
            "Iteration 9936\n",
            "1\n",
            "Iteration 9937\n",
            "1\n",
            "Iteration 9938\n",
            "1\n",
            "Iteration 9939\n",
            "1\n",
            "Iteration 9940\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9941\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9942\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9943\n",
            "1\n",
            "Iteration 9944\n",
            "1\n",
            "Iteration 9945\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9946\n",
            "1\n",
            "Iteration 9947\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9948\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9949\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9950\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9951\n",
            "1\n",
            "Iteration 9952\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9953\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9954\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9955\n",
            "1\n",
            "Iteration 9956\n",
            "1\n",
            "Iteration 9957\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9958\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9959\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9960\n",
            "1\n",
            "Iteration 9961\n",
            "1\n",
            "Iteration 9962\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9963\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9964\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9965\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9966\n",
            "1\n",
            "Iteration 9967\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9968\n",
            "1\n",
            "Iteration 9969\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9970\n",
            "1\n",
            "Iteration 9971\n",
            "1\n",
            "Iteration 9972\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9973\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9974\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9975\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9976\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9977\n",
            "1\n",
            "Iteration 9978\n",
            "1\n",
            "Iteration 9979\n",
            "1\n",
            "Iteration 9980\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9981\n",
            "1\n",
            "Iteration 9982\n",
            "1\n",
            "Iteration 9983\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9984\n",
            "1\n",
            "Iteration 9985\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9986\n",
            "1\n",
            "Iteration 9987\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9988\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9989\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9990\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9991\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9992\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9993\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 9994\n",
            "1\n",
            "Iteration 9995\n",
            "1\n",
            "Iteration 9996\n",
            "1\n",
            "Iteration 9997\n",
            "1\n",
            "Iteration 9998\n",
            "1\n",
            "Iteration 9999\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration 10000\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Iteration: 10000; Percent complete: 100.0%; Average loss: 5.3723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7bo4A8hTTiR"
      },
      "source": [
        "### Evaluating Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaxxbxbPdGRO",
        "outputId": "03a461d5-76f0-4bcb-9a74-7f964c75a96b"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "forward_encoder.eval()\n",
        "forward_decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(forward_encoder, forward_decoder)\n",
        "\n",
        "# Begin chatting\n",
        "evaluateInput(forward_encoder, forward_decoder, searcher, voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> hello\n",
            "Bot: hi mary . i m calling to speak to you .\n",
            "> bye\n",
            "Bot: yes . . ? .\n",
            "> q\n"
          ]
        }
      ]
    }
  ]
}